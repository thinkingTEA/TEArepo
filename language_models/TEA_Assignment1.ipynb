{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1st assignment (n-gram language models)\n",
    "- #### Dimitris Georgiou - DS3517004\n",
    "- #### Stratos Gounidellis - DS3517005\n",
    "- #### Natasa Farmaki - DS3517018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 - Initial corpus pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package perluniprops to\n",
      "[nltk_data]     C:\\Users\\nataz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package perluniprops is already up-to-date!\n",
      "[nltk_data] Downloading package perluniprops to\n",
      "[nltk_data]     C:\\Users\\nataz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package perluniprops is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import the necessary libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('perluniprops')\n",
    "import re\n",
    "import pprint\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize.moses import MosesDetokenizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "\n",
    "# the text used as corpus for the project\n",
    "file = \"europarl-v7.ro-en.en\"\n",
    "\n",
    "text_list = []\n",
    "\n",
    "# read the whole the text\n",
    "with open(file, encoding=\"utf8\") as file:\n",
    "    for line in file:\n",
    "        text_list.append(line)\n",
    "\n",
    "# split the text into sentences and randomly select the 33% of them as test set\n",
    "train, test = train_test_split(text_list, test_size=0.33, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code block, the text for the bigram model and the trigram model is constructed. The main procedure that takes place is the addition of the tokens \"start\" and \"end\" at the beginning and the end of each sentence respectively. Moreover, the words appearing less than ten times are replaced with the token \"*UNK*\". Finally, the constructed texts are saved into the files \"unigram.txt\", \"bigram.txt\" and \"trigram.txt\". The whole process takes more or less 6 hours in a normal computer, so <b>it would be advised not to run the following code</b> . The .txt files have already been created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# tokenize the text and calculate the frequency of each token\n",
    "text = \" \".join(train)\n",
    "text = text.replace(\"\\n\", \"*new_line*\")\n",
    "tokens = word_tokenize(text)\n",
    "fdist = nltk.FreqDist(tokens)\n",
    "\n",
    "# replace the least frequent tokens\n",
    "# (i.e. tokens appearing less than 10 times with *UNK*)\n",
    "tokens_less10 = [k for k, v in fdist.items() if v <= 10]\n",
    "tokens = [i if i not in tokens_less10 else \"*UNK*\" for i in tokens]\n",
    "\n",
    "# reconstruct the text, with the least frequent words replaced\n",
    "detokenizer = MosesDetokenizer()\n",
    "text = detokenizer.detokenize(tokens, return_str=True)\n",
    "text = text.replace(\"*new_line*\", \"\\n\")\n",
    "text_file = open(\"unigram.txt\", \"w\", encoding=\"utf8\")\n",
    "text_file.write(text)\n",
    "text_file.close()\n",
    "\n",
    "# read again the reconstructed text\n",
    "train = []\n",
    "for line in text.split(\"\\n\"):\n",
    "    train.append(line.strip())\n",
    "\n",
    "# build the bigrams by adding start1 token at the begining of each sentence\n",
    "# and end12 at the end of each sentence\n",
    "# build the trigrams by adding start1 start2 tokens at the begining of each\n",
    "# sentence and end12 at the end of each sentence\n",
    "bigram_text = \"\"\n",
    "trigram_text = \"\"\n",
    "for line in train:\n",
    "    if len(line) >= 2 and line[-2] == \".\":\n",
    "        bigram_text += \"*start1* \" + line[:-2] + \" *end12*\"\n",
    "        trigram_text += \"*start1* *start2* \" + line[:-2] + \" *end12*\"\n",
    "    else:\n",
    "        bigram_text += \"*start1* \" + line + \" *end12*\"\n",
    "        trigram_text += \"*start1* *start2* \" + line + \" *end12*\"\n",
    "\n",
    "# save the results in text files\n",
    "text_file = open(\"bigram.txt\", \"w\", encoding=\"utf8\")\n",
    "text_file.write(bigram_text)\n",
    "text_file.close()\n",
    "\n",
    "text_file = open(\"trigram.txt\", \"w\", encoding=\"utf8\")\n",
    "text_file.write(trigram_text)\n",
    "text_file.close()\n",
    "\n",
    "# tokenize the bigrams\n",
    "tokens = tokenizer.tokenize(bigram_text)\n",
    "bgs = nltk.bigrams(tokens)\n",
    "fdist_bgs = nltk.FreqDist(bgs)\n",
    "\n",
    "# tokenize the trigrams\n",
    "tokens = tokenizer.tokenize(trigram_text)\n",
    "tgs = nltk.trigrams(tokens)\n",
    "fdist_tgs = nltk.FreqDist(tgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kneser - Ney Algorithm Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code snippet implement the necessary preprocessing for the Kneser-Ney algorithm. More specifically, we construct the bigrams and their frequencies as well as the trigrams and their frequencies. In addition, some other useful info are extracted and three dataframes are created, one for unigrams, one for bigrams and one for trigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# read the file with bigram adjusted text\n",
    "bigram_file = open(\"bigram.txt\", \"r\", encoding=\"utf8\")\n",
    "bigram_text = bigram_file.read()\n",
    "\n",
    "# tokeize the bigram text and create the bigrams\n",
    "tokens = tokenizer.tokenize(bigram_text)\n",
    "bgs = nltk.bigrams(tokens)\n",
    "\n",
    "# calculate the frequencies of the bigrams\n",
    "fdist_bgs = nltk.FreqDist(bgs)\n",
    "fdist = nltk.FreqDist(tokens)\n",
    "fdist.pop('start1', None)\n",
    "fdist.pop('end12', None)\n",
    "\n",
    "# repeat the same process for the trigram adjusted text\n",
    "trigram_file = open(\"trigram.txt\", \"r\", encoding=\"utf8\")\n",
    "trigram_text = trigram_file.read()\n",
    "tokens = tokenizer.tokenize(trigram_text)\n",
    "tgs = nltk.trigrams(tokens)\n",
    "fdist_tgs = nltk.FreqDist(tgs)\n",
    "\n",
    "# initalize a dataframe with the necessary info for the bigrams\n",
    "df_bigram = pd.DataFrame(list(fdist_bgs.items()), columns=[\"bigram\", 'count'])\n",
    "# one column with the first word of the bigram\n",
    "df_bigram[\"first_word\"] = [x[0] for x in fdist_bgs]\n",
    "# one column with the second word of the bigram\n",
    "df_bigram[\"second_word\"] = [x[1] for x in fdist_bgs]\n",
    "# sort the dataframe on the first word of the bigram\n",
    "df_bigram = df_bigram.sort_values(by=[\"first_word\"])\n",
    "df_bigram.reset_index(inplace=True)\n",
    "\n",
    "# initalize a dataframe with the necessary info for the trigrams\n",
    "df_trigram = pd.DataFrame(list(fdist_tgs.items()),\n",
    "                          columns=[\"trigram\", 'count'])\n",
    "# one column with the first word of the trigram\n",
    "df_trigram[\"first_word\"] = [x[0] for x in fdist_tgs]\n",
    "# one column with the second word of the trigram\n",
    "df_trigram[\"second_word\"] = [x[1] for x in fdist_tgs]\n",
    "# one column with the third word of the trigram\n",
    "df_trigram[\"third_word\"] = [x[2] for x in fdist_tgs]\n",
    "# column with a tuple containing the first and the second word of the trigram\n",
    "df_trigram[\"pre\"] = [x[0:2] for x in fdist_tgs]\n",
    "# column with a tuple containing the second and the third word of the trigram\n",
    "df_trigram[\"post\"] = [x[1:3] for x in fdist_tgs]\n",
    "# sort the dataframe on the first word of the trigram\n",
    "df_trigram = df_trigram.sort_values(by=[\"first_word\"])\n",
    "\n",
    "# initalize a dataframe with the necessary info for the unigrams\n",
    "df_unigram = pd.DataFrame(list(fdist.items()), columns=[\"unigram\", 'count'])\n",
    "# sort the dataframe on the unigram\n",
    "df_unigram = df_unigram.sort_values(by=['unigram'])\n",
    "df_unigram.reset_index(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def addKNUnigram(df_unigram, df_bigram, df_trigram, test, D=0.75):\n",
    "    \n",
    "    # tokenize the test, i.e. the sentence for which the smoothed probability will be calculated\n",
    "    unigrams_test = tokenizer.tokenize(test)\n",
    "    \n",
    "    # if a token is not found in the unigrams of the training set replace it with \"UNK\"\n",
    "    unigrams_test = [t if t in df_unigram.unigram.values else \"UNK\" for t in unigrams_test]\n",
    "    \n",
    "    # calculate the frequencies of the tokens\n",
    "    fdist = nltk.FreqDist(unigrams_test)\n",
    "    \n",
    "    # add the token start1 at the begining of the test sentence and the token end12 at the end\n",
    "    test_bgs = 'start1 ' + test.strip() + ' end12'\n",
    "    # create the bigrams\n",
    "    line_tokens_bgs = tokenizer.tokenize(test_bgs)\n",
    "    line_tokens_bgs = [t if (t in df_unigram.unigram.values or t in [\"start1\", \"end12\"]) else \"UNK\" for t in line_tokens_bgs]\n",
    "    # create the bigrams\n",
    "    bigrams_test = nltk.bigrams(line_tokens_bgs)\n",
    "    fdist_bgs = nltk.FreqDist(bigrams_test)\n",
    "    \n",
    "    # add the tokens start1 start2 at the begining of the test sentence and the token end12 at the end\n",
    "    test_tgs = 'start1 start2 ' + test.strip() + ' end12'\n",
    "    line_tokens_tgs = tokenizer.tokenize(test_tgs)\n",
    "    line_tokens_tgs = [t if (t in df_unigram.unigram.values or t in [\"start1\", \"end12\", \"start2\"]) else \"UNK\" for t in line_tokens_tgs]\n",
    "    # create the trigrams\n",
    "    trigrams_test = nltk.trigrams(line_tokens_tgs)\n",
    "    fdist_tgs = nltk.FreqDist(trigrams_test)\n",
    "\n",
    "    unigrams_test_df = pd.DataFrame(unigrams_test)    \n",
    "    bigrams_test_df = pd.DataFrame(bigrams_test)\n",
    "    trigrams_test_df = pd.DataFrame(trigrams_test)\n",
    "    \n",
    "    # create subsets of the trainig dataframes containing information\n",
    "    # only for the tokens, which exist in the test sentence\n",
    "    sub_unigram = df_unigram[df_unigram[\"unigram\"].isin(unigrams_test)].copy()\n",
    "    sub_bigram = df_bigram[df_bigram[\"bigram\"].isin(fdist_bgs.keys())].copy()\n",
    "    sub_trigram = pd.DataFrame(columns=[\"trigram\", 'count', 'first_word', 'second_word', 'third_word', 'pre', 'post'])\n",
    "    \n",
    "    for i in fdist_tgs.keys():\n",
    "        for j in df_trigram[\"trigram\"]:\n",
    "            if i == j:\n",
    "                sub_trigram = sub_trigram.append(df_trigram[df_trigram[\"trigram\"] == j])\n",
    "    #sub_trigram = df_trigram[df_trigram[\"trigram\"].isin(list(fdist_tgs.keys()))].copy()\n",
    "    df_uni_final = pd.DataFrame(columns=[\"unigram\", 'count'])\n",
    "    df_bgs_final = pd.DataFrame(columns=[\"bigram\", 'count', 'first_word', 'second_word'])\n",
    "    df_tgs_final = pd.DataFrame(columns=[\"trigram\", 'count', 'first_word', 'second_word', 'third_word', 'pre', 'post'])\n",
    "    \n",
    "    for i in unigrams_test:\n",
    "        df_uni_final = df_uni_final.append(sub_unigram[sub_unigram.unigram == i])\n",
    "    \n",
    "    for i in list(nltk.bigrams(line_tokens_bgs)):\n",
    "        if i in list(sub_bigram.bigram):\n",
    "            df_bgs_final = df_bgs_final.append(sub_bigram[sub_bigram.bigram == i])\n",
    "        else:\n",
    "            df_bgs_final = df_bgs_final.append({\"bigram\":i, \"count\":0, \"first_word\":i[0], \"second_word\":i[1]}, ignore_index=True)  \n",
    "\n",
    "    for i in list(nltk.trigrams(line_tokens_tgs)):\n",
    "        if i in list(sub_trigram.trigram):\n",
    "            \n",
    "            df_tgs_final = df_tgs_final.append(sub_trigram[sub_trigram.trigram == i])\n",
    "        else:\n",
    "            df_tgs_final = df_tgs_final.append({\"trigram\":i, \"count\":0, \"first_word\":i[0], \"second_word\":i[1],\n",
    "                                                \"third_word\":i[2], \"pre\":(i[0], i[1]),\n",
    "                                                \"post\":(i[1], i[2])}, ignore_index=True)\n",
    "    \n",
    "    sub_unigram = df_uni_final.copy()\n",
    "    sub_bigram = df_bgs_final.copy()    \n",
    "    sub_trigram = df_tgs_final.copy()\n",
    "\n",
    "    bigrams = len(df_bigram)\n",
    "    temp_df2 = df_bigram.copy()\n",
    "    \n",
    "    # NWordDot, how many bigrams begin with specific word\n",
    "    NWordDot = pd.DataFrame(columns=[\"WordDot\", 'count'])\n",
    "    # NDotWord, how many bigrams end with the specific word\n",
    "    NDotWord = pd.DataFrame(columns=[\"DotWord\", 'count'])\n",
    "    for b in list(sub_bigram.bigram):\n",
    "        # how many bigram begin with that word\n",
    "        NWordDot = NWordDot.append({\"WordDot\":b[0], \"count\": temp_df2[temp_df2.first_word==b[0]].groupby(['first_word']).size().values}, ignore_index=True)\n",
    "        # how many bigram end with that word\n",
    "        NDotWord = NDotWord.append({\"DotWord\":b[1], \"count\": temp_df2[temp_df2.second_word==b[1]].groupby(['second_word']).size().values}, ignore_index=True)\n",
    "    \n",
    "    # NDotWordDot, how many trigrams have that word as second word\n",
    "    NDotWordDot = pd.DataFrame(columns=[\"DotWordDot\", 'count'])\n",
    "    temp_df3 = df_trigram.copy()\n",
    "\n",
    "    for t in list(sub_trigram.trigram):\n",
    "        # NDotWordDot, in how many trigrams the word is in the middle\n",
    "        NDotWordDot = NDotWordDot.append({\"DotWordDot\":t[1], \"count\": temp_df3[temp_df3.second_word==t[1]].groupby(['second_word']).size().values}, ignore_index=True)\n",
    "\n",
    "    # remove from the dataframes the enries containing the tokens\n",
    "    # start1, start2, end12\n",
    "    NDotWordDot = NDotWordDot[NDotWordDot.DotWordDot!=\"start2\"]\n",
    "    NDotWordDot = NDotWordDot[NDotWordDot.DotWordDot!=\"start1\"]\n",
    "    NDotWordDot = NDotWordDot[NDotWordDot.DotWordDot!=\"end12\"]\n",
    "    NWordDot = NWordDot[NWordDot.WordDot!=\"end12\"]\n",
    "    NWordDot = NWordDot[NWordDot.WordDot!=\"start1\"]\n",
    "    NWordDot = NWordDot[NWordDot.WordDot!=\"start2\"]\n",
    "    NDotWord = NDotWord[NDotWord.DotWord!=\"end12\"]\n",
    "    NDotWord = NDotWord[NDotWord.DotWord!=\"start1\"]\n",
    "    NDotWord = NDotWord[NDotWord.DotWord!=\"start2\"]\n",
    "    \n",
    "    # calculate parts of the Kneser-Ney algorithm and \n",
    "    # save thoses calculated fiels in the sub_unigram dataframe\n",
    "    sub_unigram[\"Pcont\"] = (NDotWord[\"count\"]/float(bigrams)).values\n",
    "    sub_unigram.loc[:, 'Pcont'] = sub_unigram.Pcont.map(lambda x: x[0])\n",
    "    sub_unigram[\"lambda\"] = ((float(D)*NWordDot[\"count\"]) / NDotWordDot[\"count\"]).values\n",
    "    sub_unigram.loc[:, 'lambda'] = sub_unigram[\"lambda\"].map(lambda x: x[0])\n",
    "    sub_unigram[\"NDotWordDot\"] = NDotWordDot[\"count\"].values\n",
    "    sub_unigram.loc[:, 'NDotWordDot'] = sub_unigram.NDotWordDot.map(lambda x: x[0])\n",
    "    sub_unigram[\"NWordDot\"] = NWordDot[\"count\"].values\n",
    "    sub_unigram.loc[:, 'NWordDot'] = sub_unigram.NWordDot.map(lambda x: x[0])\n",
    "\n",
    "    sub_unigram.fillna(0, inplace=True)\n",
    "    #sub_unigram = sub_unigram.sort_values(by=['unigram'])\n",
    "    sub_unigram.reset_index(inplace=True)\n",
    "    del sub_unigram[\"index\"]\n",
    "    del sub_unigram[\"level_0\"]\n",
    "    \n",
    "    # return the subsets of the training data\n",
    "    return(sub_unigram, sub_bigram, sub_trigram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def addKNBigram(df_unigram, df_bigram, df_trigram, sub_unigram, sub_bigram, sub_trigram, D=0.75):\n",
    "\n",
    "    temp_df3 = df_trigram.copy()\n",
    "    # NDotW1W2, how many trigrams end with those two specific words\n",
    "    NDotW1W2 = pd.DataFrame(columns=[\"DotW1W2\", 'count'])\n",
    "    # NDotW1W2, how many trigrams begin with those two specific words\n",
    "    NW1W2Dot = pd.DataFrame(columns=[\"W1W2Dot\", 'count'])\n",
    "    \n",
    "    for t in list(sub_trigram.trigram):\n",
    "        NDotW1W2 = NDotW1W2.append({\"DotW1W2\": (t[1], t[2]), \"count\": len(temp_df3[temp_df3.post==(t[1], t[2])].groupby(['post']).size().values)}, ignore_index=True)\n",
    "        NW1W2Dot = NW1W2Dot.append({\"W1W2Dot\": (t[0], t[1]), \"count\": len(temp_df3[temp_df3.post==(t[0], t[1])].groupby(['pre']).size().values)}, ignore_index=True)\n",
    "    \n",
    "    unigram_temp = sub_unigram.copy()\n",
    "    sub_bigram[\"mod_count\"] = sub_bigram[\"count\"]\n",
    "    stopWords = [\"start1\", \"end12\"]\n",
    "    for index, row in sub_bigram.iterrows():\n",
    "        # if a test bigram is not found in the training set, then its count is replaced by the count of its first word\n",
    "        if row[\"mod_count\"] == 0:\n",
    "            if row[\"first_word\"] in stopWords:\n",
    "                sub_bigram.loc[index, \"mod_count\"] = df_unigram[df_unigram[\"unigram\"] == \"UNK\"][\"count\"].values[0]\n",
    "            else:\n",
    "                sub_bigram.loc[index, \"mod_count\"] = sub_unigram[sub_unigram[\"unigram\"] == row[\"first_word\"]][\"count\"].values[0]\n",
    "    #sub_bigram.reset_index(inplace=True)\n",
    "    \n",
    "    # adjust the dataframes to have the same length, in order\n",
    "    # to make the calculations correctly\n",
    "    temp_bgs = sub_bigram[:-1]\n",
    "    temp_ndw1w2dot = NW1W2Dot.iloc[1:]\n",
    "    #temp_bgs.reset_index(inplace=True)\n",
    "    \n",
    "    # implement some initial calculations for the algorithm\n",
    "    lambda_2 = list((D /temp_bgs[\"mod_count\"]).values * temp_ndw1w2dot[\"count\"].values)\n",
    "    lambda_2.append(None)\n",
    "    sub_bigram[\"lambda2\"] = lambda_2\n",
    "    unigram_temp.set_index('unigram', inplace=True)\n",
    "    trigram_temp = sub_trigram.copy()\n",
    "    \n",
    "    for index, row in sub_bigram.iterrows():\n",
    "        w1 = row[\"first_word\"]\n",
    "        w2 = row[\"second_word\"]\n",
    "        if w2 not in stopWords and w1 not in stopWords:\n",
    "            # extract the necessary parts for the calculations\n",
    "            nDotW1W2 = pd.Series(NDotW1W2[NDotW1W2[\"DotW1W2\"] == (w1, w2)][\"count\"]).values[0]\n",
    "            nDotWordDot = pd.Series(unigram_temp.loc[w2][\"NDotWordDot\"]).values[0]\n",
    "            lambda_bgs = pd.Series(unigram_temp.loc[w2][\"lambda\"]).values[0]\n",
    "            pcont_bgs = pd.Series(unigram_temp.loc[w2][\"Pcont\"]).values[0]\n",
    "            cW1W2 = pd.Series(sub_bigram.loc[index, \"mod_count\"]).values[0]\n",
    "            cW1 = pd.Series(unigram_temp.loc[w1][\"count\"]).values[0]\n",
    "            nWordDot = pd.Series(unigram_temp.loc[w1][\"NWordDot\"]).values[0]\n",
    "            \n",
    "            if nDotWordDot == 0:\n",
    "                sub_bigram.loc[index, \"Pcont2\"]  = 0\n",
    "                sub_bigram.loc[index, \"KNSmoothing_BGS\"]  = 0\n",
    "            else:\n",
    "                sub_bigram.loc[index, \"Pcont2\"] = (max(nDotW1W2 - D, 0.0)/nDotWordDot) + lambda_bgs * pcont_bgs\n",
    "                if (pd.Series(sub_bigram.loc[index, \"count\"]).values[0] == 0):\n",
    "                    # if a bigram is not found in trainig set, calculate its probability following \n",
    "                    # process similar to Laplace. Add to the denominato the number of the unigrams, in \n",
    "                    # order to make the whole probability smaller.\n",
    "                    sub_bigram.loc[index, \"KNSmoothing_BGS\"] = (max(cW1W2 - D, 0.0)/(cW1 + len(df_unigram))) + (D * nWordDot/cW1 * cW1W2/len(df_bigram))\n",
    "                else:\n",
    "                    sub_bigram.loc[index, \"KNSmoothing_BGS\"] = (max(cW1W2 - D, 0.0)/cW1) + (D * nWordDot/cW1 * cW1W2/len(df_bigram))\n",
    "    #del sub_bigram[\"level_0\"]\n",
    "    #del sub_bigram[\"index\"]\n",
    "    \n",
    "    # return the updated dataframe\n",
    "    return(sub_bigram)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def addKNTrigram(df_unigram, df_bigram, df_trigram, sub_unigram, sub_bigram, sub_trigram, D=0.75):\n",
    "\n",
    "    bigram_temp = sub_bigram.copy()\n",
    "    stopWords = [\"start1\", \"start2\", \"end12\"]\n",
    "    NlambdaW1W2 = pd.DataFrame(columns=[\"W1W2\", 'lambdaW1W2'])\n",
    "    NprobW2W3 = pd.DataFrame(columns=[\"W2W3\", 'probW2W3'])\n",
    "\n",
    "    for index, row in sub_trigram.iterrows():\n",
    "            pre = row[\"pre\"]\n",
    "            post = row[\"post\"]\n",
    "            w1 = row[\"first_word\"]\n",
    "            w2 = row[\"second_word\"]\n",
    "            w3 = row[\"third_word\"]\n",
    "            cW1W2W3 = pd.Series(row[\"count\"]).values[0]\n",
    "            \n",
    "            if w2 not in stopWords and w1 not in stopWords and w3 not in stopWords:\n",
    "                cW1W2 = pd.Series(bigram_temp[bigram_temp.bigram==pre][\"mod_count\"]).values[0]\n",
    "                cW2 = pd.Series(sub_unigram[sub_unigram.unigram==w2][\"count\"]).values[0]\n",
    "                if cW1W2 == 0 or cW1W2W3 == 0:\n",
    "                    # if a trigram is not found in the training set then as count is\n",
    "                    # used the count of the middle word, while in the denominator it is also\n",
    "                    # added the number of the bigrams\n",
    "                    sub_trigram.loc[index, \"MaxLikelTerm\"] = max(cW2-D,0)/(len(df_bigram) + cW2)\n",
    "                else:\n",
    "                    sub_trigram.loc[index, \"MaxLikelTerm\"]  = max(cW1W2W3-D,0)/cW1W2\n",
    "\n",
    "    temp_lambda2 = bigram_temp[[\"bigram\", \"lambda2\"]]\n",
    "    data = []\n",
    "    data.insert(0, {'bigram': '(start1, start2)', 'lambda2': None})\n",
    "\n",
    "    temp_lambda2= pd.concat([pd.DataFrame(data), temp_lambda2], ignore_index=True)\n",
    "    temp_lambda2 = temp_lambda2[:-1]\n",
    "    temp_Pcont2 = bigram_temp[[\"bigram\", \"Pcont2\"]]\n",
    "    \n",
    "    # calculate the smoothed probabilty for the trigram model\n",
    "    sub_trigram[\"KNSmoothing_TGS\"] = sub_trigram[\"MaxLikelTerm\"].values + temp_lambda2[\"lambda2\"].values * temp_Pcont2[\"Pcont2\"].values\n",
    "    return(sub_trigram)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 - Check the log-probabilities\n",
    "\n",
    "We compare the log-probabilities of correct sentences as far as structure is concerned with sentences randomly generated. In general, the correctly structured sentences should be more probable and from the results it is obvious that this happens almost always in the trigram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>correct_sentence</th>\n",
       "      <th>logProb_cs</th>\n",
       "      <th>wrong_sentence</th>\n",
       "      <th>logProb_ws</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yes, I totally agree: let us set challenging t...</td>\n",
       "      <td>-71.970115</td>\n",
       "      <td>let us agree set challenging compliance with u...</td>\n",
       "      <td>-49.730257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The allocation of the budget to the Member Sta...</td>\n",
       "      <td>-161.105469</td>\n",
       "      <td>the into for The the Member non take financial...</td>\n",
       "      <td>-179.010287</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    correct_sentence  logProb_cs  \\\n",
       "0  Yes, I totally agree: let us set challenging t...  -71.970115   \n",
       "1  The allocation of the budget to the Member Sta... -161.105469   \n",
       "\n",
       "                                      wrong_sentence  logProb_ws  \n",
       "0  let us agree set challenging compliance with u...  -49.730257  \n",
       "1  the into for The the Member non take financial... -179.010287  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>correct_sentence</th>\n",
       "      <th>logProb_cs</th>\n",
       "      <th>wrong_sentence</th>\n",
       "      <th>logProb_ws</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yes, I totally agree: let us set challenging t...</td>\n",
       "      <td>-61.003979</td>\n",
       "      <td>but compliance challenging confuse totally tar...</td>\n",
       "      <td>-85.942526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The allocation of the budget to the Member Sta...</td>\n",
       "      <td>-126.407836</td>\n",
       "      <td>well to for The Member financial and the capac...</td>\n",
       "      <td>-146.292515</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    correct_sentence  logProb_cs  \\\n",
       "0  Yes, I totally agree: let us set challenging t...  -61.003979   \n",
       "1  The allocation of the budget to the Member Sta... -126.407836   \n",
       "\n",
       "                                      wrong_sentence  logProb_ws  \n",
       "0  but compliance challenging confuse totally tar...  -85.942526  \n",
       "1  well to for The Member financial and the capac... -146.292515  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check the log-probabilities that the trained models return when  (correct) sentences\n",
    "# from the test subset are given vs. (incorrect) sentences of the same length (in words)\n",
    "# consisting of randomly selected vocabulary words.\n",
    "\n",
    "def eval_bigram(test):\n",
    "    detokenizer = MosesDetokenizer()\n",
    "    testdf = pd.DataFrame(columns=[\"correct_sentence\",\"logProb_cs\",\"wrong_sentence\",\"logProb_ws\"])\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    for sentence in test:\n",
    "        test_tokenized = tokenizer.tokenize(sentence)\n",
    "        random.shuffle(test_tokenized)\n",
    "        random_test = detokenizer.detokenize(test_tokenized, return_str=True)\n",
    "        correct_uni = (addKNUnigram (df_unigram, df_bigram, df_trigram, sentence))\n",
    "        correct_bgs = (addKNBigram (df_unigram, df_bigram, df_trigram, correct_uni[0], correct_uni[1], correct_uni[2]))\n",
    "\n",
    "        wrong_uni = (addKNUnigram (df_unigram, df_bigram, df_trigram, random_test))\n",
    "        wrong_bgs = (addKNBigram (df_unigram, df_bigram, df_trigram, wrong_uni[0], wrong_uni[1], wrong_uni[2]))\n",
    "        prob_ws = sum(np.log(wrong_bgs[\"KNSmoothing_BGS\"].dropna(axis=0, how='all')))\n",
    "        prob_cs = sum(np.log(correct_bgs[\"KNSmoothing_BGS\"].dropna(axis=0, how='all')))\n",
    "        \n",
    "        testdf = testdf.append({\"correct_sentence\": sentence, \"logProb_cs\": prob_cs,\n",
    "                                \"wrong_sentence\": random_test, \"logProb_ws\": prob_ws},\n",
    "                               ignore_index=True)\n",
    "    display(testdf)\n",
    "    \n",
    "def eval_trigram(test):\n",
    "    detokenizer = MosesDetokenizer()\n",
    "    testdf = pd.DataFrame(columns=[\"correct_sentence\",\"logProb_cs\",\"wrong_sentence\",\"logProb_ws\"])\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    for sentence in test:\n",
    "        \n",
    "        test_tokenized = tokenizer.tokenize(sentence)\n",
    "        random.shuffle(test_tokenized)\n",
    "        random_test = detokenizer.detokenize(test_tokenized, return_str=True)\n",
    "        correct_uni = (addKNUnigram (df_unigram, df_bigram, df_trigram, sentence))\n",
    "        correct_bgs = (addKNBigram (df_unigram, df_bigram, df_trigram, correct_uni[0], correct_uni[1], correct_uni[2]))\n",
    "        correct_tgs = (addKNTrigram (df_unigram, df_bigram, df_trigram, correct_uni[0], correct_bgs, correct_uni[2]))\n",
    "        \n",
    "        wrong_uni = (addKNUnigram (df_unigram, df_bigram, df_trigram, random_test))\n",
    "        wrong_bgs = (addKNBigram (df_unigram, df_bigram, df_trigram, wrong_uni[0], wrong_uni[1], wrong_uni[2]))\n",
    "        wrong_tgs = (addKNTrigram (df_unigram, df_bigram, df_trigram, wrong_uni[0], wrong_bgs, wrong_uni[2]))\n",
    "        prob_ws = sum(np.log(wrong_tgs[\"KNSmoothing_TGS\"].dropna(axis=0, how='all')))\n",
    "        prob_cs = sum(np.log(correct_tgs[\"KNSmoothing_TGS\"].dropna(axis=0, how='all')))\n",
    "        \n",
    "        testdf = testdf.append({\"correct_sentence\": sentence, \"logProb_cs\": prob_cs,\n",
    "                                \"wrong_sentence\": random_test, \"logProb_ws\": prob_ws},\n",
    "                               ignore_index=True)\n",
    "        \n",
    "    display(testdf)\n",
    "\n",
    "eval_bigram(test[:25])\n",
    "eval_trigram(test[:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 - Predictive keyboard\n",
    "\n",
    "The aim of the following code snippet is to predict the next word as in a predictive keyboard. Given a sentence we focus mainly on the last part of it (i.e mostly the last four words) and we predict the next word. If the last token does not exist in the vocabulary of the the trained model we utilize the edit distance and among the closest words the most probable bigrams or trigrams are chosen. Although we implemented edit distance we used the implementation of nltk for efficiency reasons and we just set the substitution cost to two. With that approach we simulate better real case scenarios. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The above models could be used to predict the next (vocabulary) word, as in a predictive keyboard\n",
    "\n",
    "# the method returns the ten most probable bigrams begining with the given word\n",
    "def build_bigrams(next_word, df_bigram):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokenized = tokenizer.tokenize(next_word)\n",
    "    tokenized = tokenized[-1]\n",
    "    sub_bigram = df_bigram[(df_bigram.first_word == tokenized) & (df_bigram.second_word != \"UNK\")  & (df_bigram.second_word != \"end12\")][[\"bigram\", \"count\"]]\n",
    "    sub_bigram.sort_values('count', ascending=False, inplace=True)\n",
    "    sub_bigram = sub_bigram.head(10)\n",
    "    sub_bigram = sub_bigram[\"bigram\"]\n",
    "    sub_bigram = sub_bigram.apply(lambda x: (' '.join(x)))\n",
    "    return sub_bigram\n",
    "\n",
    "# the methods returns the ten most probable trigrams begining with the given words\n",
    "def build_trigrams(next_word, df_trigram):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokenized = tokenizer.tokenize(next_word)\n",
    "    tokenized = tokenized[-2:]\n",
    "    sub_trigram = df_trigram[(df_trigram.first_word == tokenized[0]) & (df_trigram.second_word == tokenized[1])  & (df_trigram.third_word != \"UNK\")  & (df_bigram.second_word != \"end12\")][[\"trigram\", \"count\"]]\n",
    "    \n",
    "    sub_trigram.sort_values('count', ascending=False, inplace=True)\n",
    "    sub_trigram = sub_trigram.head(10)\n",
    "    sub_trigram = sub_trigram[\"trigram\"]\n",
    "    sub_trigram = sub_trigram.apply(lambda x: (' '.join(x)))\n",
    "    return sub_trigram\n",
    "\n",
    "# the method calculates the top three most probable words in the given context\n",
    "# In order to achieve that the models built above are used. The smoothed probabilities\n",
    "# are calculated for different n-grams, and the words resulting in the highest probability\n",
    "# are chosen.\n",
    "def pred_next_word(next_word, df_bigram, df_trigram, df_unigram):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokenized = tokenizer.tokenize(next_word)\n",
    "    \n",
    "    pred_words = pd.DataFrame(columns=[\"n-gram\",\"logProb\"])\n",
    "    if (len(tokenized) > 0):\n",
    "        next_bigram = build_bigrams(next_word, df_bigram)\n",
    "        for index, row in next_bigram.iteritems():\n",
    "            correct_uni = (addKNUnigram (df_unigram, df_bigram, df_trigram, row))\n",
    "            correct_bgs = (addKNBigram (df_unigram, df_bigram, df_trigram, correct_uni[0], correct_uni[1], correct_uni[2]))\n",
    "            prob_cs = sum(np.log(correct_bgs[\"KNSmoothing_BGS\"].dropna(axis=0, how='all')))\n",
    "            pred_words = pred_words.append({\"n-gram\": row, \"logProb\": prob_cs},\n",
    "                                  ignore_index=True)\n",
    "        if len(tokenized) > 1:\n",
    "            next_trigram = build_trigrams(next_word, df_trigram)\n",
    "            for index, row in next_trigram.iteritems():\n",
    "                correct_uni = (addKNUnigram (df_unigram, df_bigram, df_trigram, row))\n",
    "                correct_bgs = (addKNBigram (df_unigram, df_bigram, df_trigram, correct_uni[0], correct_uni[1], correct_uni[2]))\n",
    "                correct_tgs = (addKNTrigram (df_unigram, df_bigram, df_trigram, correct_uni[0], correct_bgs, correct_uni[2]))\n",
    "                prob_cs = sum(np.log(correct_tgs[\"KNSmoothing_TGS\"].dropna(axis=0, how='all')))\n",
    "                pred_words = pred_words.append({\"n-gram\": row, \"logProb\": prob_cs},\n",
    "                                       ignore_index=True)\n",
    "    tokenized = tokenized[-1]\n",
    "    pred_words.sort_values('logProb', ascending=False, inplace=True)\n",
    "    df_unigram.sort_values('count', ascending=False, inplace=True)\n",
    "    pred_words = pred_words.head(5)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    top_words = set()\n",
    "    for index, row in pred_words.iterrows(): \n",
    "        tokenized = tokenizer.tokenize(row[\"n-gram\"])\n",
    "        top_words.add(tokenized[-1])\n",
    "    dist_words = pd.DataFrame(columns=[\"word\",\"dist\"])\n",
    "    for index, row in df_unigram.iterrows():\n",
    "        dist_words = dist_words.append({\"word\":row[\"unigram\"], \"dist\": nltk.edit_distance(row[\"unigram\"], tokenized)},\n",
    "                                      ignore_index=True)\n",
    "    dist_words.sort_values('dist', ascending=True, inplace=True)\n",
    "    dist_words = pd.DataFrame(dist_words.head(3 - len(top_words)))\n",
    "\n",
    "    for index, row in dist_words.iterrows():\n",
    "        if len(top_words) < 3:\n",
    "            top_words.add(row[\"word\"])\n",
    "    print(\"Predictions: \", top_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nataz\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions {'Union', 'Court', 'Council'}"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nataz\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions {'Commission', 'Union', 'Parliament'}\n"
     ]
    }
   ],
   "source": [
    "# The above models could be used to predict the next (vocabulary) word, as in a predictive keyboard.\n",
    "# However, the above approach works if the last word exists in the vocabulary. If the word does not exist,\n",
    "# the following approach is proposed, where the Levenshtein distance (edit - distance) is calculated. Then, among\n",
    "# the closest words the most probable combinations/n-grams are chosen. This case is more generic and more realistic.\n",
    "\n",
    "def build_bigrams_edit(next_word, df_bigram):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokenized = tokenizer.tokenize(next_word)\n",
    "    sub_bigram = df_bigram[(df_bigram.first_word == tokenized[-2]) & (df_bigram.second_word != \"UNK\")  & (df_bigram.second_word != \"end12\")][[\"bigram\",\"count\", \"second_word\"]]\n",
    "    sub_bigram.sort_values('count', ascending=False, inplace=True)\n",
    "    sub_bigram = sub_bigram.head(30)\n",
    "    for index, row in sub_bigram.iterrows():\n",
    "        sub_bigram.loc[index, \"dist\"] = nltk.edit_distance(row[\"second_word\"], tokenized[-1], substitution_cost=2)\n",
    "    sub_bigram.sort_values('dist', ascending=True, inplace=True)    \n",
    "    sub_bigram = sub_bigram.head(10)\n",
    "    sub_bigram = sub_bigram[\"bigram\"]\n",
    "    sub_bigram = sub_bigram.apply(lambda x: (' '.join(x)))\n",
    "    return sub_bigram  \n",
    "\n",
    "\n",
    "def build_trigrams_edit(next_word, df_trigram):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokenized = tokenizer.tokenize(next_word)\n",
    "    sub_trigram = df_trigram[(df_trigram.first_word == tokenized[-3]) & (df_trigram.second_word == tokenized[-2])  & (df_trigram.third_word != \"UNK\")  & (df_bigram.second_word != \"end12\")][[\"trigram\", \"count\", \"third_word\"]]\n",
    "    sub_trigram.sort_values('count', ascending=False, inplace=True)\n",
    "    sub_trigram = sub_trigram.head(30)\n",
    "    for index, row in sub_trigram.iterrows():\n",
    "        sub_trigram.loc[index, \"dist\"] = nltk.edit_distance(row[\"third_word\"], tokenized[-1], substitution_cost=2)\n",
    "    sub_trigram.sort_values('dist', ascending=True, inplace=True)    \n",
    "    sub_trigram = sub_trigram.head(10)\n",
    "    sub_trigram = sub_trigram[\"trigram\"]\n",
    "    sub_trigram = sub_trigram.apply(lambda x: (' '.join(x)))\n",
    "    return sub_trigram \n",
    "\n",
    "def pred_next_word_edit(next_word, df_bigram, df_trigram, df_unigram):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokenized = tokenizer.tokenize(next_word)\n",
    "\n",
    "    pred_words = pd.DataFrame(columns=[\"n-gram\",\"logProb\"])\n",
    "    if (len(tokenized) > 1):\n",
    "        next_bigram = build_bigrams_edit(next_word, df_bigram)\n",
    "        for index, row in next_bigram.iteritems():\n",
    "            correct_uni = (addKNUnigram (df_unigram, df_bigram, df_trigram, row))\n",
    "            correct_bgs = (addKNBigram (df_unigram, df_bigram, df_trigram, correct_uni[0], correct_uni[1], correct_uni[2]))\n",
    "            prob_cs = sum(np.log(correct_bgs[\"KNSmoothing_BGS\"].dropna(axis=0, how='all')))\n",
    "            pred_words = pred_words.append({\"n-gram\": row, \"logProb\": prob_cs},\n",
    "                                  ignore_index=True)\n",
    "        if len(tokenized) > 2:\n",
    "            next_trigram = build_trigrams_edit(next_word, df_trigram)\n",
    "            for index, row in next_trigram.iteritems():\n",
    "                correct_uni = (addKNUnigram (df_unigram, df_bigram, df_trigram, row))\n",
    "                correct_bgs = (addKNBigram (df_unigram, df_bigram, df_trigram, correct_uni[0], correct_uni[1], correct_uni[2]))\n",
    "                correct_tgs = (addKNTrigram (df_unigram, df_bigram, df_trigram, correct_uni[0], correct_bgs, correct_uni[2]))\n",
    "                prob_cs = sum(np.log(correct_tgs[\"KNSmoothing_TGS\"].dropna(axis=0, how='all')))\n",
    "                pred_words = pred_words.append({\"n-gram\": row, \"logProb\": prob_cs},\n",
    "                                       ignore_index=True)\n",
    "    tokenized = tokenized[-1]  \n",
    "    pred_words.sort_values('logProb', ascending=False, inplace=True)\n",
    "    df_unigram.sort_values('count', ascending=False, inplace=True)\n",
    "    pred_words = pred_words.head(5)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    top_words = set()\n",
    "    for index, row in pred_words.iterrows(): \n",
    "        tokenized = tokenizer.tokenize(row[\"n-gram\"])\n",
    "        top_words.add(tokenized[-1])\n",
    "    dist_words = pd.DataFrame(columns=[\"word\",\"dist\"])\n",
    "    for index, row in df_unigram.iterrows():\n",
    "        dist_words = dist_words.append({\"word\":row[\"unigram\"], \"dist\": nltk.edit_distance(row[\"unigram\"], tokenized)},\n",
    "                                      ignore_index=True)\n",
    "    dist_words.sort_values('dist', ascending=True, inplace=True)\n",
    "    dist_words = pd.DataFrame(dist_words.head(3 - len(top_words)))\n",
    "\n",
    "    for index, row in dist_words.iterrows():\n",
    "        if len(top_words) < 3:\n",
    "            top_words.add(row[\"word\"])\n",
    "        else:\n",
    "            break\n",
    "    print(\"Predictions: \", top_words)\n",
    "\n",
    "# method to check whether the last token exists in the vocabulary or not\n",
    "# if it does not call the pred_next_word_edit() method else\n",
    "# the pred_next_word() with the required arguments\n",
    "def pred_next(next_word):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokenized = tokenizer.tokenize(next_word)\n",
    "    if (tokenized[-1]) not in df_unigram.unigram.values:\n",
    "        pred_next_word_edit(next_word, df_bigram, df_trigram, df_unigram)\n",
    "    else:\n",
    "        pred_next_word(next_word, df_bigram, df_trigram, df_unigram)\n",
    "        \n",
    "\n",
    "next_word = \"the European uni\"\n",
    "pred_next(next_word)\n",
    "\n",
    "next_word = \"the European\"\n",
    "pred_next(next_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4 - Perplexity & Cross-Entropy\n",
    "\n",
    "Calculate metrics for the two different models/approaches. In both cases, i.e. as far as perplexity and cross-entropy is concerned the trigram model seems to ouperform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_string_bgs = \" \".join(test)\n",
    "correct_uni = (addKNUnigram (df_unigram, df_bigram, df_trigram, test_string_bgs))\n",
    "correct_bgs = (addKNBigram (df_unigram, df_bigram, df_trigram, correct_uni[0], correct_uni[1], correct_uni[2]))\n",
    "\n",
    "print ('Perplexity Bigram Model: ', \n",
    "       math.exp(sum(np.log(1/correct_bgs[\"KNSmoothing_BGS\"].dropna(axis=0, how='all')))/len(correct_bgs[\"KNSmoothing_BGS\"].dropna(axis=0, how='all'))))\n",
    "\n",
    "print ('Cross-Entropy Bigram Model: ',\n",
    "      sum(-1*np.log(correct_bgs[\"KNSmoothing_BGS\"].dropna(axis=0, how='all')))/len(correct_bgs[\"KNSmoothing_BGS\"].dropna(axis=0, how='all')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity 14.309804553934033\n",
      "Cross-Entropy 2.66094493547\n"
     ]
    }
   ],
   "source": [
    "test_string_tgs = \" \".join(test)\n",
    "correct_uni = (addKNUnigram (df_unigram, df_bigram, df_trigram, test_string_tgs))\n",
    "correct_bgs = (addKNBigram (df_unigram, df_bigram, df_trigram, correct_uni[0], correct_uni[1], correct_uni[2]))\n",
    "correct_tgs = (addKNTrigram (df_unigram, df_bigram, df_trigram, correct_uni[0], correct_bgs, correct_uni[2]))\n",
    "\n",
    "print ('Perplexity Trigram Model: ', \n",
    "       math.exp(sum(np.log(1/correct_tgs[\"KNSmoothing_TGS\"].dropna(axis=0, how='all')))/len(correct_tgs[\"KNSmoothing_TGS\"].dropna(axis=0, how='all'))))\n",
    "\n",
    "print ('Cross-Entropy Trigram Model: ',\n",
    "      sum(-1*np.log(correct_tgs[\"KNSmoothing_TGS\"].dropna(axis=0, how='all')))/len(correct_tgs[\"KNSmoothing_TGS\"].dropna(axis=0, how='all')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
