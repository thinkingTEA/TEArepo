{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1st assignment (n-gram language models)\n",
    "- #### Dimitris Georgiou - DS3517004\n",
    "- #### Stratos Gounidellis - DS3517005\n",
    "- #### Natasa Farmaki - DS3517018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 - Initial corpus pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nataz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package perluniprops to\n",
      "[nltk_data]     C:\\Users\\nataz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package perluniprops is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import the necessary libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('perluniprops')\n",
    "import re\n",
    "import pprint\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize.moses import MosesDetokenizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "\n",
    "# the text used as corpus for the project\n",
    "file = \"europarl-v7.ro-en.en\"\n",
    "\n",
    "text_list = []\n",
    "\n",
    "# read the whole the text\n",
    "with open(file, encoding=\"utf8\") as file:\n",
    "    for line in file:\n",
    "        text_list.append(line)\n",
    "\n",
    "# split the text into sentences and randomly select the 33% of them as test set\n",
    "train, test = train_test_split(text_list, test_size=0.33, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code block, the text for the bigram model and the trigram model is constructed. The main procedure that takes place is the addition of the tokens \"start\" and \"end\" at the beginning and the end of each sentence respectively. Moreover, the words appearing less than ten times are replaced with the token \"*UNK*\". Finally, the constructed texts are saved into the files \"unigram.txt\", \"bigram.txt\" and \"trigram.txt\". The whole process takes more or less 6 hours in a normal computer, so <b>it would be advised not to run the following code</b> . The .txt files have already been created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# tokenize the text and calculate the frequency of each token\n",
    "text = \" \".join(train)\n",
    "text = text.replace(\"\\n\", \"*new_line*\")\n",
    "tokens = word_tokenize(text)\n",
    "fdist = nltk.FreqDist(tokens)\n",
    "\n",
    "# replace the least frequent tokens\n",
    "# (i.e. tokens appearing less than 10 times with *UNK*)\n",
    "tokens_less10 = [k for k, v in fdist.items() if v <= 10]\n",
    "tokens = [i if i not in tokens_less10 else \"*UNK*\" for i in tokens]\n",
    "\n",
    "# reconstruct the text, with the least frequent words replaced\n",
    "detokenizer = MosesDetokenizer()\n",
    "text = detokenizer.detokenize(tokens, return_str=True)\n",
    "text = text.replace(\"*new_line*\", \"\\n\")\n",
    "text_file = open(\"unigram.txt\", \"w\", encoding=\"utf8\")\n",
    "text_file.write(text)\n",
    "text_file.close()\n",
    "\n",
    "# read again the reconstructed text\n",
    "train = []\n",
    "for line in text.split(\"\\n\"):\n",
    "    train.append(line.strip())\n",
    "\n",
    "# build the bigrams by adding start1 token at the begining of each sentence\n",
    "# and end12 at the end of each sentence\n",
    "# build the trigrams by adding start1 start2 tokens at the begining of each\n",
    "# sentence and end12 at the end of each sentence\n",
    "bigram_text = \"\"\n",
    "trigram_text = \"\"\n",
    "for line in train:\n",
    "    if len(line) >= 2 and line[-2] == \".\":\n",
    "        bigram_text += \"*start1* \" + line[:-2] + \" *end12*\"\n",
    "        trigram_text += \"*start1* *start2* \" + line[:-2] + \" *end12*\"\n",
    "    else:\n",
    "        bigram_text += \"*start1* \" + line + \" *end12*\"\n",
    "        trigram_text += \"*start1* *start2* \" + line + \" *end12*\"\n",
    "\n",
    "# save the results in text files\n",
    "text_file = open(\"bigram.txt\", \"w\", encoding=\"utf8\")\n",
    "text_file.write(bigram_text)\n",
    "text_file.close()\n",
    "\n",
    "text_file = open(\"trigram.txt\", \"w\", encoding=\"utf8\")\n",
    "text_file.write(trigram_text)\n",
    "text_file.close()\n",
    "\n",
    "# tokenize the bigrams\n",
    "tokens = tokenizer.tokenize(bigram_text)\n",
    "bgs = nltk.bigrams(tokens)\n",
    "fdist_bgs = nltk.FreqDist(bgs)\n",
    "\n",
    "# tokenize the trigrams\n",
    "tokens = tokenizer.tokenize(trigram_text)\n",
    "tgs = nltk.trigrams(tokens)\n",
    "fdist_tgs = nltk.FreqDist(tgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kneser - Ney Algorithm Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code snippet implement the necessary preprocessing for the Kneser-Ney algorithm. More specifically, we construct the bigrams and their frequencies as well as the trigrams and their frequencies. In addition, some other useful info are extracted and three dataframes are created, one for unigrams, one for bigrams and one for trigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# read the file with bigram adjusted text\n",
    "bigram_file = open(\"bigram.txt\", \"r\", encoding=\"utf8\")\n",
    "bigram_text = bigram_file.read()\n",
    "\n",
    "# tokeize the bigram text and create the bigrams\n",
    "tokens = tokenizer.tokenize(bigram_text)\n",
    "bgs = nltk.bigrams(tokens)\n",
    "\n",
    "# calculate the frequencies of the bigrams\n",
    "fdist_bgs = nltk.FreqDist(bgs)\n",
    "fdist = nltk.FreqDist(tokens)\n",
    "fdist.pop('start1', None)\n",
    "fdist.pop('end12', None)\n",
    "\n",
    "# repeat the same process for the trigram adjusted text\n",
    "trigram_file = open(\"trigram.txt\", \"r\", encoding=\"utf8\")\n",
    "trigram_text = trigram_file.read()\n",
    "tokens = tokenizer.tokenize(trigram_text)\n",
    "tgs = nltk.trigrams(tokens)\n",
    "fdist_tgs = nltk.FreqDist(tgs)\n",
    "\n",
    "# initalize a dataframe with the necessary info for the bigrams\n",
    "df_bigram = pd.DataFrame(list(fdist_bgs.items()), columns=[\"bigram\", 'count'])\n",
    "# one column with the first word of the bigram\n",
    "df_bigram[\"first_word\"] = [x[0] for x in fdist_bgs]\n",
    "# one column with the second word of the bigram\n",
    "df_bigram[\"second_word\"] = [x[1] for x in fdist_bgs]\n",
    "# sort the dataframe on the first word of the bigram\n",
    "df_bigram = df_bigram.sort_values(by=[\"first_word\"])\n",
    "df_bigram.reset_index(inplace=True)\n",
    "\n",
    "# initalize a dataframe with the necessary info for the trigrams\n",
    "df_trigram = pd.DataFrame(list(fdist_tgs.items()),\n",
    "                          columns=[\"trigram\", 'count'])\n",
    "# one column with the first word of the trigram\n",
    "df_trigram[\"first_word\"] = [x[0] for x in fdist_tgs]\n",
    "# one column with the second word of the trigram\n",
    "df_trigram[\"second_word\"] = [x[1] for x in fdist_tgs]\n",
    "# one column with the third word of the trigram\n",
    "df_trigram[\"third_word\"] = [x[2] for x in fdist_tgs]\n",
    "# column with a tuple containing the first and the second word of the trigram\n",
    "df_trigram[\"pre\"] = [x[0:2] for x in fdist_tgs]\n",
    "# column with a tuple containing the second and the third word of the trigram\n",
    "df_trigram[\"post\"] = [x[1:3] for x in fdist_tgs]\n",
    "# sort the dataframe on the first word of the trigram\n",
    "df_trigram = df_trigram.sort_values(by=[\"first_word\"])\n",
    "\n",
    "# initalize a dataframe with the necessary info for the unigrams\n",
    "df_unigram = pd.DataFrame(list(fdist.items()), columns=[\"unigram\", 'count'])\n",
    "# sort the dataframe on the unigram\n",
    "df_unigram = df_unigram.sort_values(by=['unigram'])\n",
    "df_unigram.reset_index(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def addKNUnigram(df_unigram, df_bigram, df_trigram, test, D=0.75):\n",
    "    \n",
    "    # tokenize the test, i.e. the sentence for which the smoothed probability will be calculated\n",
    "    unigrams_test = tokenizer.tokenize(test)\n",
    "    \n",
    "    # if a token is not found in the unigrams of the training set replace it with \"UNK\"\n",
    "    unigrams_test = [t if t in df_unigram.unigram.values else \"UNK\" for t in unigrams_test]\n",
    "    \n",
    "    # calculate the frequencies of the tokens\n",
    "    fdist = nltk.FreqDist(unigrams_test)\n",
    "    \n",
    "    # add the token start1 at the begining of the test sentence and the token end12 at the end\n",
    "    test_bgs = 'start1 ' + test.strip() + ' end12'\n",
    "    # create the bigrams\n",
    "    line_tokens_bgs = tokenizer.tokenize(test_bgs)\n",
    "    line_tokens_bgs = [t if (t in df_unigram.unigram.values or t in [\"start1\", \"end12\"]) else \"UNK\" for t in line_tokens_bgs]\n",
    "    # create the bigrams\n",
    "    bigrams_test = nltk.bigrams(line_tokens_bgs)\n",
    "    fdist_bgs = nltk.FreqDist(bigrams_test)\n",
    "    \n",
    "    # add the tokens start1 start2 at the begining of the test sentence and the token end12 at the end\n",
    "    test_tgs = 'start1 start2 ' + test.strip() + ' end12'\n",
    "    line_tokens_tgs = tokenizer.tokenize(test_tgs)\n",
    "    line_tokens_tgs = [t if (t in df_unigram.unigram.values or t in [\"start1\", \"end12\", \"start2\"]) else \"UNK\" for t in line_tokens_tgs]\n",
    "    # create the trigrams\n",
    "    trigrams_test = nltk.trigrams(line_tokens_tgs)\n",
    "    fdist_tgs = nltk.FreqDist(trigrams_test)\n",
    "\n",
    "    unigrams_test_df = pd.DataFrame(unigrams_test)    \n",
    "    bigrams_test_df = pd.DataFrame(bigrams_test)\n",
    "    trigrams_test_df = pd.DataFrame(trigrams_test)\n",
    "    \n",
    "    # create subsets of the trainig dataframes containing information\n",
    "    # only for the tokens, which exist in the test sentence\n",
    "    sub_unigram = df_unigram[df_unigram[\"unigram\"].isin(unigrams_test)].copy()\n",
    "    sub_bigram = df_bigram[df_bigram[\"bigram\"].isin(fdist_bgs.keys())].copy()\n",
    "    sub_trigram = pd.DataFrame(columns=[\"trigram\", 'count', 'first_word', 'second_word', 'third_word', 'pre', 'post'])\n",
    "    \n",
    "    for i in fdist_tgs.keys():\n",
    "        for j in df_trigram[\"trigram\"]:\n",
    "            if i == j:\n",
    "                sub_trigram = sub_trigram.append(df_trigram[df_trigram[\"trigram\"] == j])\n",
    "    #sub_trigram = df_trigram[df_trigram[\"trigram\"].isin(list(fdist_tgs.keys()))].copy()\n",
    "    df_uni_final = pd.DataFrame(columns=[\"unigram\", 'count'])\n",
    "    df_bgs_final = pd.DataFrame(columns=[\"bigram\", 'count', 'first_word', 'second_word'])\n",
    "    df_tgs_final = pd.DataFrame(columns=[\"trigram\", 'count', 'first_word', 'second_word', 'third_word', 'pre', 'post'])\n",
    "    \n",
    "    for i in unigrams_test:\n",
    "        df_uni_final = df_uni_final.append(sub_unigram[sub_unigram.unigram == i])\n",
    "    \n",
    "    for i in list(nltk.bigrams(line_tokens_bgs)):\n",
    "        if i in list(sub_bigram.bigram):\n",
    "            df_bgs_final = df_bgs_final.append(sub_bigram[sub_bigram.bigram == i])\n",
    "        else:\n",
    "            df_bgs_final = df_bgs_final.append({\"bigram\":i, \"count\":0, \"first_word\":i[0], \"second_word\":i[1]}, ignore_index=True)  \n",
    "\n",
    "    for i in list(nltk.trigrams(line_tokens_tgs)):\n",
    "        if i in list(sub_trigram.trigram):\n",
    "            \n",
    "            df_tgs_final = df_tgs_final.append(sub_trigram[sub_trigram.trigram == i])\n",
    "        else:\n",
    "            df_tgs_final = df_tgs_final.append({\"trigram\":i, \"count\":0, \"first_word\":i[0], \"second_word\":i[1],\n",
    "                                                \"third_word\":i[2], \"pre\":(i[0], i[1]),\n",
    "                                                \"post\":(i[1], i[2])}, ignore_index=True)\n",
    "    \n",
    "    sub_unigram = df_uni_final.copy()\n",
    "    sub_bigram = df_bgs_final.copy()    \n",
    "    sub_trigram = df_tgs_final.copy()\n",
    "\n",
    "    bigrams = len(df_bigram)\n",
    "    temp_df2 = df_bigram.copy()\n",
    "    \n",
    "    # NWordDot, how many bigrams begin with specific word\n",
    "    NWordDot = pd.DataFrame(columns=[\"WordDot\", 'count'])\n",
    "    # NDotWord, how many bigrams end with the specific word\n",
    "    NDotWord = pd.DataFrame(columns=[\"DotWord\", 'count'])\n",
    "    for b in list(sub_bigram.bigram):\n",
    "        # how many bigram begin with that word\n",
    "        NWordDot = NWordDot.append({\"WordDot\":b[0], \"count\": temp_df2[temp_df2.first_word==b[0]].groupby(['first_word']).size().values}, ignore_index=True)\n",
    "        # how many bigram end with that word\n",
    "        NDotWord = NDotWord.append({\"DotWord\":b[1], \"count\": temp_df2[temp_df2.second_word==b[1]].groupby(['second_word']).size().values}, ignore_index=True)\n",
    "    \n",
    "    # NDotWordDot, how many trigrams have that word as second word\n",
    "    NDotWordDot = pd.DataFrame(columns=[\"DotWordDot\", 'count'])\n",
    "    temp_df3 = df_trigram.copy()\n",
    "\n",
    "    for t in list(sub_trigram.trigram):\n",
    "        # NDotWordDot, in how many trigrams the word is in the middle\n",
    "        NDotWordDot = NDotWordDot.append({\"DotWordDot\":t[1], \"count\": temp_df3[temp_df3.second_word==t[1]].groupby(['second_word']).size().values}, ignore_index=True)\n",
    "\n",
    "    # remove from the dataframes the enries containing the tokens\n",
    "    # start1, start2, end12\n",
    "    NDotWordDot = NDotWordDot[NDotWordDot.DotWordDot!=\"start2\"]\n",
    "    NDotWordDot = NDotWordDot[NDotWordDot.DotWordDot!=\"start1\"]\n",
    "    NDotWordDot = NDotWordDot[NDotWordDot.DotWordDot!=\"end12\"]\n",
    "    NWordDot = NWordDot[NWordDot.WordDot!=\"end12\"]\n",
    "    NWordDot = NWordDot[NWordDot.WordDot!=\"start1\"]\n",
    "    NWordDot = NWordDot[NWordDot.WordDot!=\"start2\"]\n",
    "    NDotWord = NDotWord[NDotWord.DotWord!=\"end12\"]\n",
    "    NDotWord = NDotWord[NDotWord.DotWord!=\"start1\"]\n",
    "    NDotWord = NDotWord[NDotWord.DotWord!=\"start2\"]\n",
    "    \n",
    "    # calculate parts of the Kneser-Ney algorithm and \n",
    "    # save thoses calculated fiels in the sub_unigram dataframe\n",
    "    sub_unigram[\"Pcont\"] = (NDotWord[\"count\"]/float(bigrams)).values\n",
    "    sub_unigram.loc[:, 'Pcont'] = sub_unigram.Pcont.map(lambda x: x[0])\n",
    "    sub_unigram[\"lambda\"] = ((float(D)*NWordDot[\"count\"]) / NDotWordDot[\"count\"]).values\n",
    "    sub_unigram.loc[:, 'lambda'] = sub_unigram[\"lambda\"].map(lambda x: x[0])\n",
    "    sub_unigram[\"NDotWordDot\"] = NDotWordDot[\"count\"].values\n",
    "    sub_unigram.loc[:, 'NDotWordDot'] = sub_unigram.NDotWordDot.map(lambda x: x[0])\n",
    "    sub_unigram[\"NWordDot\"] = NWordDot[\"count\"].values\n",
    "    sub_unigram.loc[:, 'NWordDot'] = sub_unigram.NWordDot.map(lambda x: x[0])\n",
    "\n",
    "    sub_unigram.fillna(0, inplace=True)\n",
    "    #sub_unigram = sub_unigram.sort_values(by=['unigram'])\n",
    "    sub_unigram.reset_index(inplace=True)\n",
    "    del sub_unigram[\"index\"]\n",
    "    del sub_unigram[\"level_0\"]\n",
    "    \n",
    "    # return the subsets of the training data\n",
    "    return(sub_unigram, sub_bigram, sub_trigram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def addKNBigram(df_unigram, df_bigram, df_trigram, sub_unigram, sub_bigram, sub_trigram, D=0.75):\n",
    "\n",
    "    temp_df3 = df_trigram.copy()\n",
    "    # NDotW1W2, how many trigrams end with those two specific words\n",
    "    NDotW1W2 = pd.DataFrame(columns=[\"DotW1W2\", 'count'])\n",
    "    # NDotW1W2, how many trigrams begin with those two specific words\n",
    "    NW1W2Dot = pd.DataFrame(columns=[\"W1W2Dot\", 'count'])\n",
    "    \n",
    "    for t in list(sub_trigram.trigram):\n",
    "        NDotW1W2 = NDotW1W2.append({\"DotW1W2\": (t[1], t[2]), \"count\": len(temp_df3[temp_df3.post==(t[1], t[2])].groupby(['post']).size().values)}, ignore_index=True)\n",
    "        NW1W2Dot = NW1W2Dot.append({\"W1W2Dot\": (t[0], t[1]), \"count\": len(temp_df3[temp_df3.post==(t[0], t[1])].groupby(['pre']).size().values)}, ignore_index=True)\n",
    "    \n",
    "    unigram_temp = sub_unigram.copy()\n",
    "    sub_bigram[\"mod_count\"] = sub_bigram[\"count\"]\n",
    "    stopWords = [\"start1\", \"end12\"]\n",
    "    for index, row in sub_bigram.iterrows():\n",
    "        # if a test bigram is not found in the training set, then its count is replaced by the count of its first word\n",
    "        if row[\"mod_count\"] == 0:\n",
    "            if row[\"first_word\"] in stopWords:\n",
    "                sub_bigram.loc[index, \"mod_count\"] = df_unigram[df_unigram[\"unigram\"] == \"UNK\"][\"count\"].values[0]\n",
    "            else:\n",
    "                sub_bigram.loc[index, \"mod_count\"] = sub_unigram[sub_unigram[\"unigram\"] == row[\"first_word\"]][\"count\"].values[0]\n",
    "    #sub_bigram.reset_index(inplace=True)\n",
    "    \n",
    "    # adjust the dataframes to have the same length, in order\n",
    "    # to make the calculations correctly\n",
    "    temp_bgs = sub_bigram[:-1]\n",
    "    temp_ndw1w2dot = NW1W2Dot.iloc[1:]\n",
    "    #temp_bgs.reset_index(inplace=True)\n",
    "    \n",
    "    # implement some initial calculations for the algorithm\n",
    "    lambda_2 = list((D /temp_bgs[\"mod_count\"]).values * temp_ndw1w2dot[\"count\"].values)\n",
    "    lambda_2.append(None)\n",
    "    sub_bigram[\"lambda2\"] = lambda_2\n",
    "    unigram_temp.set_index('unigram', inplace=True)\n",
    "    trigram_temp = sub_trigram.copy()\n",
    "    \n",
    "    for index, row in sub_bigram.iterrows():\n",
    "        w1 = row[\"first_word\"]\n",
    "        w2 = row[\"second_word\"]\n",
    "        if w2 not in stopWords and w1 not in stopWords:\n",
    "            # extract the necessary parts for the calculations\n",
    "            nDotW1W2 = pd.Series(NDotW1W2[NDotW1W2[\"DotW1W2\"] == (w1, w2)][\"count\"]).values[0]\n",
    "            nDotWordDot = pd.Series(unigram_temp.loc[w2][\"NDotWordDot\"]).values[0]\n",
    "            lambda_bgs = pd.Series(unigram_temp.loc[w2][\"lambda\"]).values[0]\n",
    "            pcont_bgs = pd.Series(unigram_temp.loc[w2][\"Pcont\"]).values[0]\n",
    "            cW1W2 = pd.Series(sub_bigram.loc[index, \"mod_count\"]).values[0]\n",
    "            cW1 = pd.Series(unigram_temp.loc[w1][\"count\"]).values[0]\n",
    "            nWordDot = pd.Series(unigram_temp.loc[w1][\"NWordDot\"]).values[0]\n",
    "            \n",
    "            if nDotWordDot == 0:\n",
    "                sub_bigram.loc[index, \"Pcont2\"]  = 0\n",
    "                sub_bigram.loc[index, \"KNSmoothing_BGS\"]  = 0\n",
    "            else:\n",
    "                sub_bigram.loc[index, \"Pcont2\"] = (max(nDotW1W2 - D, 0.0)/nDotWordDot) + lambda_bgs * pcont_bgs\n",
    "                if (pd.Series(sub_bigram.loc[index, \"count\"]).values[0] == 0):\n",
    "                    # if a bigram is not found in trainig set, calculate its probability following \n",
    "                    # process similar to Laplace. Add to the denominato the number of the unigrams, in \n",
    "                    # order to make the whole probability smaller.\n",
    "                    sub_bigram.loc[index, \"KNSmoothing_BGS\"] = (max(cW1W2 - D, 0.0)/(cW1 + len(df_unigram))) + (D * nWordDot/cW1 * cW1W2/len(df_bigram))\n",
    "                else:\n",
    "                    sub_bigram.loc[index, \"KNSmoothing_BGS\"] = (max(cW1W2 - D, 0.0)/cW1) + (D * nWordDot/cW1 * cW1W2/len(df_bigram))\n",
    "    #del sub_bigram[\"level_0\"]\n",
    "    #del sub_bigram[\"index\"]\n",
    "    \n",
    "    # return the updated dataframe\n",
    "    return(sub_bigram)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def addKNTrigram(df_unigram, df_bigram, df_trigram, sub_unigram, sub_bigram, sub_trigram, D=0.75):\n",
    "\n",
    "    bigram_temp = sub_bigram.copy()\n",
    "    stopWords = [\"start1\", \"start2\", \"end12\"]\n",
    "    NlambdaW1W2 = pd.DataFrame(columns=[\"W1W2\", 'lambdaW1W2'])\n",
    "    NprobW2W3 = pd.DataFrame(columns=[\"W2W3\", 'probW2W3'])\n",
    "\n",
    "    for index, row in sub_trigram.iterrows():\n",
    "            pre = row[\"pre\"]\n",
    "            post = row[\"post\"]\n",
    "            w1 = row[\"first_word\"]\n",
    "            w2 = row[\"second_word\"]\n",
    "            w3 = row[\"third_word\"]\n",
    "            cW1W2W3 = pd.Series(row[\"count\"]).values[0]\n",
    "            \n",
    "            if w2 not in stopWords and w1 not in stopWords and w3 not in stopWords:\n",
    "                cW1W2 = pd.Series(bigram_temp[bigram_temp.bigram==pre][\"mod_count\"]).values[0]\n",
    "                cW2 = pd.Series(sub_unigram[sub_unigram.unigram==w2][\"count\"]).values[0]\n",
    "                if cW1W2 == 0 or cW1W2W3 == 0:\n",
    "                    # if a trigram is not found in the training set then as count is\n",
    "                    # used the count of the middle word, while in the denominator it is also\n",
    "                    # added the number of the bigrams\n",
    "                    sub_trigram.loc[index, \"MaxLikelTerm\"] = max(cW2-D,0)/(len(df_bigram) + cW2)\n",
    "                else:\n",
    "                    sub_trigram.loc[index, \"MaxLikelTerm\"]  = max(cW1W2W3-D,0)/cW1W2\n",
    "\n",
    "    temp_lambda2 = bigram_temp[[\"bigram\", \"lambda2\"]]\n",
    "    data = []\n",
    "    data.insert(0, {'bigram': '(start1, start2)', 'lambda2': None})\n",
    "\n",
    "    temp_lambda2= pd.concat([pd.DataFrame(data), temp_lambda2], ignore_index=True)\n",
    "    temp_lambda2 = temp_lambda2[:-1]\n",
    "    temp_Pcont2 = bigram_temp[[\"bigram\", \"Pcont2\"]]\n",
    "    \n",
    "    # calculate the smoothed probabilty for the trigram model\n",
    "    sub_trigram[\"KNSmoothing_TGS\"] = sub_trigram[\"MaxLikelTerm\"].values + temp_lambda2[\"lambda2\"].values * temp_Pcont2[\"Pcont2\"].values\n",
    "    return(sub_trigram)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 - Check the log-probabilities\n",
    "\n",
    "We compare the log-probabilities of correct sentences as far as structure is concerned with sentences randomly generated. In general, the correctly structured sentences should be more probable and from the results it is obvious that this happens almost always in the trigram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>correct_sentence</th>\n",
       "      <th>logProb_cs</th>\n",
       "      <th>wrong_sentence</th>\n",
       "      <th>logProb_ws</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yes, I totally agree: let us set challenging t...</td>\n",
       "      <td>-71.970115</td>\n",
       "      <td>let I confuse with compliance but let totally ...</td>\n",
       "      <td>-74.484186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The allocation of the budget to the Member Sta...</td>\n",
       "      <td>-161.105469</td>\n",
       "      <td>every country cofinancing The Member different...</td>\n",
       "      <td>-167.246107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In Greece, the dangers come from the exploitat...</td>\n",
       "      <td>-83.185418</td>\n",
       "      <td>in from catchment the dangers exploitation Bul...</td>\n",
       "      <td>-84.050367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Has that been checked, before an emergency inc...</td>\n",
       "      <td>-41.121926</td>\n",
       "      <td>been incident that Has before checked emergenc...</td>\n",
       "      <td>-25.796982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>That was, I think, one of the most important s...</td>\n",
       "      <td>-233.773617</td>\n",
       "      <td>most talking about important to has that a mad...</td>\n",
       "      <td>-295.163526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>In order to ensure that there is no misunderst...</td>\n",
       "      <td>-135.855826</td>\n",
       "      <td>order use like fossil to for In is and I our e...</td>\n",
       "      <td>-181.456244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>We are delighted that we will be welcoming a S...</td>\n",
       "      <td>-116.676106</td>\n",
       "      <td>will once a South be Joint we Sudanese has Ass...</td>\n",
       "      <td>-118.811617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>We have to revert back to peace mediation with...</td>\n",
       "      <td>-48.881705</td>\n",
       "      <td>We mediation without or revert peace to winner...</td>\n",
       "      <td>-41.737315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(HU) Ladies and gentlemen, in the course of it...</td>\n",
       "      <td>-142.581948</td>\n",
       "      <td>Central wound of crisis 2008 as its into in 20...</td>\n",
       "      <td>-153.908244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>It is also worth emphasising the role played b...</td>\n",
       "      <td>-114.506859</td>\n",
       "      <td>by development is areas also role by and in em...</td>\n",
       "      <td>-147.419297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Hence we all - MEPs and ministers in the regio...</td>\n",
       "      <td>-87.093408</td>\n",
       "      <td>and federal all are we the feel and behind reg...</td>\n",
       "      <td>-108.355207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Rather, they bring clear, measurable benefits ...</td>\n",
       "      <td>-33.085304</td>\n",
       "      <td>citizens benefits clear measurable bring Rathe...</td>\n",
       "      <td>-27.681346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>in writing. - (NL) The Dutch People's Party fo...</td>\n",
       "      <td>-95.421112</td>\n",
       "      <td>2012 for The NL an is People and Democracy s a...</td>\n",
       "      <td>-108.621463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>It is left to the Member States and peer revie...</td>\n",
       "      <td>-42.381679</td>\n",
       "      <td>It peer Member the monitoring is States suppor...</td>\n",
       "      <td>-80.838691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>However, of what use are the euro and the Euro...</td>\n",
       "      <td>-102.494285</td>\n",
       "      <td>promote the if use and and they are of do resp...</td>\n",
       "      <td>-94.669496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>In the current economic context, we could desc...</td>\n",
       "      <td>-212.477361</td>\n",
       "      <td>the of describe Greece country in context of o...</td>\n",
       "      <td>-162.078378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>The German Government is currently conducting ...</td>\n",
       "      <td>-139.892821</td>\n",
       "      <td>can its we and multiannual Government so end d...</td>\n",
       "      <td>-128.057071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>I would also like to thank the President of th...</td>\n",
       "      <td>-29.519224</td>\n",
       "      <td>to I the European would the of thank President...</td>\n",
       "      <td>-60.740351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>The vote will take place today at 11:30.\\n</td>\n",
       "      <td>-32.925108</td>\n",
       "      <td>30 11 vote today The will at place take</td>\n",
       "      <td>-29.891449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>This issue cannot be tackled appropriately by ...</td>\n",
       "      <td>-83.973544</td>\n",
       "      <td>be a be discussed appropriately cannot by alon...</td>\n",
       "      <td>-81.203198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>(SL) I am in favour of Croatia's membership of...</td>\n",
       "      <td>-90.167675</td>\n",
       "      <td>favour Union of but am SL I European of Croati...</td>\n",
       "      <td>-128.724221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Finally, it is important for us to bear in min...</td>\n",
       "      <td>-94.718968</td>\n",
       "      <td>mind the safety other to the of coal branches ...</td>\n",
       "      <td>-96.385654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Moreover, I very much appreciate Turkey's posi...</td>\n",
       "      <td>-45.145911</td>\n",
       "      <td>Turkey very positive appreciate the in role Ca...</td>\n",
       "      <td>-45.756501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>However, to quote a popular Hungarian saying, ...</td>\n",
       "      <td>-105.913375</td>\n",
       "      <td>unless horseshoes as quote a Hungarian is dead...</td>\n",
       "      <td>-130.420393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>The President of the Republic of Lithuania too...</td>\n",
       "      <td>-63.718232</td>\n",
       "      <td>by amendments immediately tabling Lithuania Re...</td>\n",
       "      <td>-47.424333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     correct_sentence  logProb_cs  \\\n",
       "0   Yes, I totally agree: let us set challenging t...  -71.970115   \n",
       "1   The allocation of the budget to the Member Sta... -161.105469   \n",
       "2   In Greece, the dangers come from the exploitat...  -83.185418   \n",
       "3   Has that been checked, before an emergency inc...  -41.121926   \n",
       "4   That was, I think, one of the most important s... -233.773617   \n",
       "5   In order to ensure that there is no misunderst... -135.855826   \n",
       "6   We are delighted that we will be welcoming a S... -116.676106   \n",
       "7   We have to revert back to peace mediation with...  -48.881705   \n",
       "8   (HU) Ladies and gentlemen, in the course of it... -142.581948   \n",
       "9   It is also worth emphasising the role played b... -114.506859   \n",
       "10  Hence we all - MEPs and ministers in the regio...  -87.093408   \n",
       "11  Rather, they bring clear, measurable benefits ...  -33.085304   \n",
       "12  in writing. - (NL) The Dutch People's Party fo...  -95.421112   \n",
       "13  It is left to the Member States and peer revie...  -42.381679   \n",
       "14  However, of what use are the euro and the Euro... -102.494285   \n",
       "15  In the current economic context, we could desc... -212.477361   \n",
       "16  The German Government is currently conducting ... -139.892821   \n",
       "17  I would also like to thank the President of th...  -29.519224   \n",
       "18         The vote will take place today at 11:30.\\n  -32.925108   \n",
       "19  This issue cannot be tackled appropriately by ...  -83.973544   \n",
       "20  (SL) I am in favour of Croatia's membership of...  -90.167675   \n",
       "21  Finally, it is important for us to bear in min...  -94.718968   \n",
       "22  Moreover, I very much appreciate Turkey's posi...  -45.145911   \n",
       "23  However, to quote a popular Hungarian saying, ... -105.913375   \n",
       "24  The President of the Republic of Lithuania too...  -63.718232   \n",
       "\n",
       "                                       wrong_sentence  logProb_ws  \n",
       "0   let I confuse with compliance but let totally ...  -74.484186  \n",
       "1   every country cofinancing The Member different... -167.246107  \n",
       "2   in from catchment the dangers exploitation Bul...  -84.050367  \n",
       "3   been incident that Has before checked emergenc...  -25.796982  \n",
       "4   most talking about important to has that a mad... -295.163526  \n",
       "5   order use like fossil to for In is and I our e... -181.456244  \n",
       "6   will once a South be Joint we Sudanese has Ass... -118.811617  \n",
       "7   We mediation without or revert peace to winner...  -41.737315  \n",
       "8   Central wound of crisis 2008 as its into in 20... -153.908244  \n",
       "9   by development is areas also role by and in em... -147.419297  \n",
       "10  and federal all are we the feel and behind reg... -108.355207  \n",
       "11  citizens benefits clear measurable bring Rathe...  -27.681346  \n",
       "12  2012 for The NL an is People and Democracy s a... -108.621463  \n",
       "13  It peer Member the monitoring is States suppor...  -80.838691  \n",
       "14  promote the if use and and they are of do resp...  -94.669496  \n",
       "15  the of describe Greece country in context of o... -162.078378  \n",
       "16  can its we and multiannual Government so end d... -128.057071  \n",
       "17  to I the European would the of thank President...  -60.740351  \n",
       "18            30 11 vote today The will at place take  -29.891449  \n",
       "19  be a be discussed appropriately cannot by alon...  -81.203198  \n",
       "20  favour Union of but am SL I European of Croati... -128.724221  \n",
       "21  mind the safety other to the of coal branches ...  -96.385654  \n",
       "22  Turkey very positive appreciate the in role Ca...  -45.756501  \n",
       "23  unless horseshoes as quote a Hungarian is dead... -130.420393  \n",
       "24  by amendments immediately tabling Lithuania Re...  -47.424333  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>correct_sentence</th>\n",
       "      <th>logProb_cs</th>\n",
       "      <th>wrong_sentence</th>\n",
       "      <th>logProb_ws</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yes, I totally agree: let us set challenging t...</td>\n",
       "      <td>-61.003979</td>\n",
       "      <td>targets confuse agree not let but let set comp...</td>\n",
       "      <td>-85.747118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The allocation of the budget to the Member Sta...</td>\n",
       "      <td>-126.407836</td>\n",
       "      <td>into of every budget cohesion take the capacit...</td>\n",
       "      <td>-143.480522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In Greece, the dangers come from the exploitat...</td>\n",
       "      <td>-58.314793</td>\n",
       "      <td>dangers come basin the catchment from exploita...</td>\n",
       "      <td>-73.170688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Has that been checked, before an emergency inc...</td>\n",
       "      <td>-39.554890</td>\n",
       "      <td>Has checked incident been occurs an before eme...</td>\n",
       "      <td>-48.573466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>That was, I think, one of the most important s...</td>\n",
       "      <td>-188.840725</td>\n",
       "      <td>important been statement the that the politica...</td>\n",
       "      <td>-216.429279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>In order to ensure that there is no misunderst...</td>\n",
       "      <td>-103.991229</td>\n",
       "      <td>our to many environment I use on fossil impact...</td>\n",
       "      <td>-147.863137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>We are delighted that we will be welcoming a S...</td>\n",
       "      <td>-93.932756</td>\n",
       "      <td>South the has signed we are a We parliamentari...</td>\n",
       "      <td>-136.304019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>We have to revert back to peace mediation with...</td>\n",
       "      <td>-48.864177</td>\n",
       "      <td>losers to to back winners have without revert ...</td>\n",
       "      <td>-52.359325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(HU) Ladies and gentlemen, in the course of it...</td>\n",
       "      <td>-108.391608</td>\n",
       "      <td>gentlemen September with in wound as in was it...</td>\n",
       "      <td>-150.077836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>It is also worth emphasising the role played b...</td>\n",
       "      <td>-101.593907</td>\n",
       "      <td>worth economic sustaining areas the promoting ...</td>\n",
       "      <td>-138.408540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Hence we all - MEPs and ministers in the regio...</td>\n",
       "      <td>-83.708616</td>\n",
       "      <td>the and facts MEPs we federal feel in Hence we...</td>\n",
       "      <td>-93.226107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Rather, they bring clear, measurable benefits ...</td>\n",
       "      <td>-35.871112</td>\n",
       "      <td>clear for bring benefits citizens Rather measu...</td>\n",
       "      <td>-38.712548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>in writing. - (NL) The Dutch People's Party fo...</td>\n",
       "      <td>-58.366926</td>\n",
       "      <td>opposed s VVD in 2012 NL for the Democracy wri...</td>\n",
       "      <td>-107.855548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>It is left to the Member States and peer revie...</td>\n",
       "      <td>-46.282331</td>\n",
       "      <td>Commission peer supported by by the States rev...</td>\n",
       "      <td>-67.480427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>However, of what use are the euro and the Euro...</td>\n",
       "      <td>-67.516882</td>\n",
       "      <td>do of Eurogroup not responsibility and what if...</td>\n",
       "      <td>-87.115933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>In the current economic context, we could desc...</td>\n",
       "      <td>-190.976426</td>\n",
       "      <td>exaggeration in the without current largest ec...</td>\n",
       "      <td>-198.986092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>The German Government is currently conducting ...</td>\n",
       "      <td>-115.153562</td>\n",
       "      <td>this have can until wait so will section begin...</td>\n",
       "      <td>-156.372036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>I would also like to thank the President of th...</td>\n",
       "      <td>-14.758040</td>\n",
       "      <td>European I the also of President Commission li...</td>\n",
       "      <td>-24.868229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>The vote will take place today at 11:30.\\n</td>\n",
       "      <td>-16.166811</td>\n",
       "      <td>11 The today will place 30 take vote at</td>\n",
       "      <td>-33.292053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>This issue cannot be tackled appropriately by ...</td>\n",
       "      <td>-67.280393</td>\n",
       "      <td>cannot This be level tackled appropriately nee...</td>\n",
       "      <td>-81.799098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>(SL) I am in favour of Croatia's membership of...</td>\n",
       "      <td>-61.462528</td>\n",
       "      <td>interests SL Croatia but European membership U...</td>\n",
       "      <td>-104.813586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Finally, it is important for us to bear in min...</td>\n",
       "      <td>-62.997376</td>\n",
       "      <td>workers safety is branches in it and of other ...</td>\n",
       "      <td>-103.723818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Moreover, I very much appreciate Turkey's posi...</td>\n",
       "      <td>-39.778100</td>\n",
       "      <td>very Turkey role s positive Caucasus Moreover ...</td>\n",
       "      <td>-52.643484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>However, to quote a popular Hungarian saying, ...</td>\n",
       "      <td>-86.541864</td>\n",
       "      <td>a much will be unless as it on is quote worth ...</td>\n",
       "      <td>-106.245592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>The President of the Republic of Lithuania too...</td>\n",
       "      <td>-60.922060</td>\n",
       "      <td>The of amendments tabling Republic took of Pre...</td>\n",
       "      <td>-60.798148</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     correct_sentence  logProb_cs  \\\n",
       "0   Yes, I totally agree: let us set challenging t...  -61.003979   \n",
       "1   The allocation of the budget to the Member Sta... -126.407836   \n",
       "2   In Greece, the dangers come from the exploitat...  -58.314793   \n",
       "3   Has that been checked, before an emergency inc...  -39.554890   \n",
       "4   That was, I think, one of the most important s... -188.840725   \n",
       "5   In order to ensure that there is no misunderst... -103.991229   \n",
       "6   We are delighted that we will be welcoming a S...  -93.932756   \n",
       "7   We have to revert back to peace mediation with...  -48.864177   \n",
       "8   (HU) Ladies and gentlemen, in the course of it... -108.391608   \n",
       "9   It is also worth emphasising the role played b... -101.593907   \n",
       "10  Hence we all - MEPs and ministers in the regio...  -83.708616   \n",
       "11  Rather, they bring clear, measurable benefits ...  -35.871112   \n",
       "12  in writing. - (NL) The Dutch People's Party fo...  -58.366926   \n",
       "13  It is left to the Member States and peer revie...  -46.282331   \n",
       "14  However, of what use are the euro and the Euro...  -67.516882   \n",
       "15  In the current economic context, we could desc... -190.976426   \n",
       "16  The German Government is currently conducting ... -115.153562   \n",
       "17  I would also like to thank the President of th...  -14.758040   \n",
       "18         The vote will take place today at 11:30.\\n  -16.166811   \n",
       "19  This issue cannot be tackled appropriately by ...  -67.280393   \n",
       "20  (SL) I am in favour of Croatia's membership of...  -61.462528   \n",
       "21  Finally, it is important for us to bear in min...  -62.997376   \n",
       "22  Moreover, I very much appreciate Turkey's posi...  -39.778100   \n",
       "23  However, to quote a popular Hungarian saying, ...  -86.541864   \n",
       "24  The President of the Republic of Lithuania too...  -60.922060   \n",
       "\n",
       "                                       wrong_sentence  logProb_ws  \n",
       "0   targets confuse agree not let but let set comp...  -85.747118  \n",
       "1   into of every budget cohesion take the capacit... -143.480522  \n",
       "2   dangers come basin the catchment from exploita...  -73.170688  \n",
       "3   Has checked incident been occurs an before eme...  -48.573466  \n",
       "4   important been statement the that the politica... -216.429279  \n",
       "5   our to many environment I use on fossil impact... -147.863137  \n",
       "6   South the has signed we are a We parliamentari... -136.304019  \n",
       "7   losers to to back winners have without revert ...  -52.359325  \n",
       "8   gentlemen September with in wound as in was it... -150.077836  \n",
       "9   worth economic sustaining areas the promoting ... -138.408540  \n",
       "10  the and facts MEPs we federal feel in Hence we...  -93.226107  \n",
       "11  clear for bring benefits citizens Rather measu...  -38.712548  \n",
       "12  opposed s VVD in 2012 NL for the Democracy wri... -107.855548  \n",
       "13  Commission peer supported by by the States rev...  -67.480427  \n",
       "14  do of Eurogroup not responsibility and what if...  -87.115933  \n",
       "15  exaggeration in the without current largest ec... -198.986092  \n",
       "16  this have can until wait so will section begin... -156.372036  \n",
       "17  European I the also of President Commission li...  -24.868229  \n",
       "18            11 The today will place 30 take vote at  -33.292053  \n",
       "19  cannot This be level tackled appropriately nee...  -81.799098  \n",
       "20  interests SL Croatia but European membership U... -104.813586  \n",
       "21  workers safety is branches in it and of other ... -103.723818  \n",
       "22  very Turkey role s positive Caucasus Moreover ...  -52.643484  \n",
       "23  a much will be unless as it on is quote worth ... -106.245592  \n",
       "24  The of amendments tabling Republic took of Pre...  -60.798148  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check the log-probabilities that the trained models return when  (correct) sentences\n",
    "# from the test subset are given vs. (incorrect) sentences of the same length (in words)\n",
    "# consisting of randomly selected vocabulary words.\n",
    "\n",
    "def eval_bigram(test):\n",
    "    detokenizer = MosesDetokenizer()\n",
    "    testdf = pd.DataFrame(columns=[\"correct_sentence\",\"logProb_cs\",\"wrong_sentence\",\"logProb_ws\"])\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    for sentence in test:\n",
    "        test_tokenized = tokenizer.tokenize(sentence)\n",
    "        random.shuffle(test_tokenized)\n",
    "        random_test = detokenizer.detokenize(test_tokenized, return_str=True)\n",
    "        correct_uni = (addKNUnigram (df_unigram, df_bigram, df_trigram, sentence))\n",
    "        correct_bgs = (addKNBigram (df_unigram, df_bigram, df_trigram, correct_uni[0], correct_uni[1], correct_uni[2]))\n",
    "\n",
    "        wrong_uni = (addKNUnigram (df_unigram, df_bigram, df_trigram, random_test))\n",
    "        wrong_bgs = (addKNBigram (df_unigram, df_bigram, df_trigram, wrong_uni[0], wrong_uni[1], wrong_uni[2]))\n",
    "        prob_ws = sum(np.log(wrong_bgs[\"KNSmoothing_BGS\"].dropna(axis=0, how='all')))\n",
    "        prob_cs = sum(np.log(correct_bgs[\"KNSmoothing_BGS\"].dropna(axis=0, how='all')))\n",
    "        \n",
    "        testdf = testdf.append({\"correct_sentence\": sentence, \"logProb_cs\": prob_cs,\n",
    "                                \"wrong_sentence\": random_test, \"logProb_ws\": prob_ws},\n",
    "                               ignore_index=True)\n",
    "    display(testdf)\n",
    "    \n",
    "def eval_trigram(test):\n",
    "    detokenizer = MosesDetokenizer()\n",
    "    testdf = pd.DataFrame(columns=[\"correct_sentence\",\"logProb_cs\",\"wrong_sentence\",\"logProb_ws\"])\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    for sentence in test:\n",
    "        \n",
    "        test_tokenized = tokenizer.tokenize(sentence)\n",
    "        random.shuffle(test_tokenized)\n",
    "        random_test = detokenizer.detokenize(test_tokenized, return_str=True)\n",
    "        correct_uni = (addKNUnigram (df_unigram, df_bigram, df_trigram, sentence))\n",
    "        correct_bgs = (addKNBigram (df_unigram, df_bigram, df_trigram, correct_uni[0], correct_uni[1], correct_uni[2]))\n",
    "        correct_tgs = (addKNTrigram (df_unigram, df_bigram, df_trigram, correct_uni[0], correct_bgs, correct_uni[2]))\n",
    "        \n",
    "        wrong_uni = (addKNUnigram (df_unigram, df_bigram, df_trigram, random_test))\n",
    "        wrong_bgs = (addKNBigram (df_unigram, df_bigram, df_trigram, wrong_uni[0], wrong_uni[1], wrong_uni[2]))\n",
    "        wrong_tgs = (addKNTrigram (df_unigram, df_bigram, df_trigram, wrong_uni[0], wrong_bgs, wrong_uni[2]))\n",
    "        prob_ws = sum(np.log(wrong_tgs[\"KNSmoothing_TGS\"].dropna(axis=0, how='all')))\n",
    "        prob_cs = sum(np.log(correct_tgs[\"KNSmoothing_TGS\"].dropna(axis=0, how='all')))\n",
    "        \n",
    "        testdf = testdf.append({\"correct_sentence\": sentence, \"logProb_cs\": prob_cs,\n",
    "                                \"wrong_sentence\": random_test, \"logProb_ws\": prob_ws},\n",
    "                               ignore_index=True)\n",
    "        \n",
    "    display(testdf)\n",
    "\n",
    "eval_bigram(test[:25])\n",
    "eval_trigram(test[:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 - Predictive keyboard\n",
    "\n",
    "The aim of the following code snippet is to predict the next word as in a predictive keyboard. Given a sentence we focus mainly on the last part of it (i.e mostly the last four words) and we predict the next word. If the last token does not exist in the vocabulary of the trained model we utilize the edit distance and among the closest words the most probable bigrams or trigrams are chosen. Although we implemented edit distance we used the implementation of nltk for efficiency reasons and we just set the substitution cost to two. With that approach we simulate better real case scenarios. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The above models could be used to predict the next (vocabulary) word, as in a predictive keyboard\n",
    "\n",
    "# the method returns the ten most probable bigrams begining with the given word\n",
    "def build_bigrams(next_word, df_bigram):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokenized = tokenizer.tokenize(next_word)\n",
    "    tokenized = tokenized[-1]\n",
    "    sub_bigram = df_bigram[(df_bigram.first_word == tokenized) & (df_bigram.second_word != \"UNK\")  & (df_bigram.second_word != \"end12\")][[\"bigram\", \"count\"]]\n",
    "    sub_bigram.sort_values('count', ascending=False, inplace=True)\n",
    "    sub_bigram = sub_bigram.head(10)\n",
    "    sub_bigram = sub_bigram[\"bigram\"]\n",
    "    sub_bigram = sub_bigram.apply(lambda x: (' '.join(x)))\n",
    "    return sub_bigram\n",
    "\n",
    "# the methods returns the ten most probable trigrams begining with the given words\n",
    "def build_trigrams(next_word, df_trigram):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokenized = tokenizer.tokenize(next_word)\n",
    "    tokenized = tokenized[-2:]\n",
    "    sub_trigram = df_trigram[(df_trigram.first_word == tokenized[0]) & (df_trigram.second_word == tokenized[1])  & (df_trigram.third_word != \"UNK\")  & (df_bigram.second_word != \"end12\")][[\"trigram\", \"count\"]]\n",
    "    \n",
    "    sub_trigram.sort_values('count', ascending=False, inplace=True)\n",
    "    sub_trigram = sub_trigram.head(10)\n",
    "    sub_trigram = sub_trigram[\"trigram\"]\n",
    "    sub_trigram = sub_trigram.apply(lambda x: (' '.join(x)))\n",
    "    return sub_trigram\n",
    "\n",
    "# the method calculates the top three most probable words in the given context\n",
    "# In order to achieve that the models built above are used. The smoothed probabilities\n",
    "# are calculated for different n-grams, and the words resulting in the highest probability\n",
    "# are chosen.\n",
    "def pred_next_word(next_word, df_bigram, df_trigram, df_unigram):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokenized = tokenizer.tokenize(next_word)\n",
    "    \n",
    "    pred_words = pd.DataFrame(columns=[\"n-gram\",\"logProb\"])\n",
    "    if (len(tokenized) > 0):\n",
    "        next_bigram = build_bigrams(next_word, df_bigram)\n",
    "        for index, row in next_bigram.iteritems():\n",
    "            correct_uni = (addKNUnigram (df_unigram, df_bigram, df_trigram, row))\n",
    "            correct_bgs = (addKNBigram (df_unigram, df_bigram, df_trigram, correct_uni[0], correct_uni[1], correct_uni[2]))\n",
    "            prob_cs = sum(np.log(correct_bgs[\"KNSmoothing_BGS\"].dropna(axis=0, how='all')))\n",
    "            pred_words = pred_words.append({\"n-gram\": row, \"logProb\": prob_cs},\n",
    "                                  ignore_index=True)\n",
    "        if len(tokenized) > 1:\n",
    "            next_trigram = build_trigrams(next_word, df_trigram)\n",
    "            for index, row in next_trigram.iteritems():\n",
    "                correct_uni = (addKNUnigram (df_unigram, df_bigram, df_trigram, row))\n",
    "                correct_bgs = (addKNBigram (df_unigram, df_bigram, df_trigram, correct_uni[0], correct_uni[1], correct_uni[2]))\n",
    "                correct_tgs = (addKNTrigram (df_unigram, df_bigram, df_trigram, correct_uni[0], correct_bgs, correct_uni[2]))\n",
    "                prob_cs = sum(np.log(correct_tgs[\"KNSmoothing_TGS\"].dropna(axis=0, how='all')))\n",
    "                pred_words = pred_words.append({\"n-gram\": row, \"logProb\": prob_cs},\n",
    "                                       ignore_index=True)\n",
    "    tokenized = tokenized[-1]\n",
    "    pred_words.sort_values('logProb', ascending=False, inplace=True)\n",
    "    df_unigram.sort_values('count', ascending=False, inplace=True)\n",
    "    pred_words = pred_words.head(5)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    top_words = set()\n",
    "    for index, row in pred_words.iterrows(): \n",
    "        tokenized = tokenizer.tokenize(row[\"n-gram\"])\n",
    "        top_words.add(tokenized[-1])\n",
    "    dist_words = pd.DataFrame(columns=[\"word\",\"dist\"])\n",
    "    for index, row in df_unigram.iterrows():\n",
    "        dist_words = dist_words.append({\"word\":row[\"unigram\"], \"dist\": nltk.edit_distance(row[\"unigram\"], tokenized)},\n",
    "                                      ignore_index=True)\n",
    "    dist_words.sort_values('dist', ascending=True, inplace=True)\n",
    "    dist_words = pd.DataFrame(dist_words.head(3 - len(top_words)))\n",
    "\n",
    "    for index, row in dist_words.iterrows():\n",
    "        if len(top_words) < 3:\n",
    "            top_words.add(row[\"word\"])\n",
    "    print(\"Predictions: \", top_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nataz\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:  {'Union', 'Council', 'Court'}"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nataz\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions:  {'Union', 'Parliament', 'Commission'}\n"
     ]
    }
   ],
   "source": [
    "# The above models could be used to predict the next (vocabulary) word, as in a predictive keyboard.\n",
    "# However, the above approach works if the last word exists in the vocabulary. If the word does not exist,\n",
    "# the following approach is proposed, where the Levenshtein distance (edit - distance) is calculated. Then, among\n",
    "# the closest words the most probable combinations/n-grams are chosen. This case is more generic and more realistic.\n",
    "\n",
    "def build_bigrams_edit(next_word, df_bigram):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokenized = tokenizer.tokenize(next_word)\n",
    "    sub_bigram = df_bigram[(df_bigram.first_word == tokenized[-2]) & (df_bigram.second_word != \"UNK\")  & (df_bigram.second_word != \"end12\")][[\"bigram\",\"count\", \"second_word\"]]\n",
    "    sub_bigram.sort_values('count', ascending=False, inplace=True)\n",
    "    sub_bigram = sub_bigram.head(30)\n",
    "    for index, row in sub_bigram.iterrows():\n",
    "        sub_bigram.loc[index, \"dist\"] = nltk.edit_distance(row[\"second_word\"], tokenized[-1], substitution_cost=2)\n",
    "    sub_bigram.sort_values('dist', ascending=True, inplace=True)    \n",
    "    sub_bigram = sub_bigram.head(10)\n",
    "    sub_bigram = sub_bigram[\"bigram\"]\n",
    "    sub_bigram = sub_bigram.apply(lambda x: (' '.join(x)))\n",
    "    return sub_bigram  \n",
    "\n",
    "\n",
    "def build_trigrams_edit(next_word, df_trigram):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokenized = tokenizer.tokenize(next_word)\n",
    "    sub_trigram = df_trigram[(df_trigram.first_word == tokenized[-3]) & (df_trigram.second_word == tokenized[-2])  & (df_trigram.third_word != \"UNK\")  & (df_bigram.second_word != \"end12\")][[\"trigram\", \"count\", \"third_word\"]]\n",
    "    sub_trigram.sort_values('count', ascending=False, inplace=True)\n",
    "    sub_trigram = sub_trigram.head(30)\n",
    "    for index, row in sub_trigram.iterrows():\n",
    "        sub_trigram.loc[index, \"dist\"] = nltk.edit_distance(row[\"third_word\"], tokenized[-1], substitution_cost=2)\n",
    "    sub_trigram.sort_values('dist', ascending=True, inplace=True)    \n",
    "    sub_trigram = sub_trigram.head(10)\n",
    "    sub_trigram = sub_trigram[\"trigram\"]\n",
    "    sub_trigram = sub_trigram.apply(lambda x: (' '.join(x)))\n",
    "    return sub_trigram \n",
    "\n",
    "def pred_next_word_edit(next_word, df_bigram, df_trigram, df_unigram):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokenized = tokenizer.tokenize(next_word)\n",
    "\n",
    "    pred_words = pd.DataFrame(columns=[\"n-gram\",\"logProb\"])\n",
    "    if (len(tokenized) > 1):\n",
    "        next_bigram = build_bigrams_edit(next_word, df_bigram)\n",
    "        for index, row in next_bigram.iteritems():\n",
    "            correct_uni = (addKNUnigram (df_unigram, df_bigram, df_trigram, row))\n",
    "            correct_bgs = (addKNBigram (df_unigram, df_bigram, df_trigram, correct_uni[0], correct_uni[1], correct_uni[2]))\n",
    "            prob_cs = sum(np.log(correct_bgs[\"KNSmoothing_BGS\"].dropna(axis=0, how='all')))\n",
    "            pred_words = pred_words.append({\"n-gram\": row, \"logProb\": prob_cs},\n",
    "                                  ignore_index=True)\n",
    "        if len(tokenized) > 2:\n",
    "            next_trigram = build_trigrams_edit(next_word, df_trigram)\n",
    "            for index, row in next_trigram.iteritems():\n",
    "                correct_uni = (addKNUnigram (df_unigram, df_bigram, df_trigram, row))\n",
    "                correct_bgs = (addKNBigram (df_unigram, df_bigram, df_trigram, correct_uni[0], correct_uni[1], correct_uni[2]))\n",
    "                correct_tgs = (addKNTrigram (df_unigram, df_bigram, df_trigram, correct_uni[0], correct_bgs, correct_uni[2]))\n",
    "                prob_cs = sum(np.log(correct_tgs[\"KNSmoothing_TGS\"].dropna(axis=0, how='all')))\n",
    "                pred_words = pred_words.append({\"n-gram\": row, \"logProb\": prob_cs},\n",
    "                                       ignore_index=True)\n",
    "    tokenized = tokenized[-1]  \n",
    "    pred_words.sort_values('logProb', ascending=False, inplace=True)\n",
    "    df_unigram.sort_values('count', ascending=False, inplace=True)\n",
    "    pred_words = pred_words.head(5)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    top_words = set()\n",
    "    for index, row in pred_words.iterrows(): \n",
    "        tokenized = tokenizer.tokenize(row[\"n-gram\"])\n",
    "        top_words.add(tokenized[-1])\n",
    "    dist_words = pd.DataFrame(columns=[\"word\",\"dist\"])\n",
    "    for index, row in df_unigram.iterrows():\n",
    "        dist_words = dist_words.append({\"word\":row[\"unigram\"], \"dist\": nltk.edit_distance(row[\"unigram\"], tokenized)},\n",
    "                                      ignore_index=True)\n",
    "    dist_words.sort_values('dist', ascending=True, inplace=True)\n",
    "    dist_words = pd.DataFrame(dist_words.head(3 - len(top_words)))\n",
    "\n",
    "    for index, row in dist_words.iterrows():\n",
    "        if len(top_words) < 3:\n",
    "            top_words.add(row[\"word\"])\n",
    "        else:\n",
    "            break\n",
    "    print(\"Predictions: \", top_words)\n",
    "\n",
    "# method to check whether the last token exists in the vocabulary or not\n",
    "# if it does not call the pred_next_word_edit() method else\n",
    "# the pred_next_word() with the required arguments\n",
    "def pred_next(next_word):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokenized = tokenizer.tokenize(next_word)\n",
    "    if (tokenized[-1]) not in df_unigram.unigram.values:\n",
    "        pred_next_word_edit(next_word, df_bigram, df_trigram, df_unigram)\n",
    "    else:\n",
    "        pred_next_word(next_word, df_bigram, df_trigram, df_unigram)\n",
    "        \n",
    "\n",
    "next_word = \"the European uni\"\n",
    "pred_next(next_word)\n",
    "\n",
    "next_word = \"the European\"\n",
    "pred_next(next_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4 - Perplexity & Cross-Entropy\n",
    "\n",
    "Calculate metrics for the two different models/approaches. In both cases, i.e. as far as perplexity and cross-entropy is concerned the trigram model seems to ouperform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity Bigram Model:  75.19362295754806\n",
      "Cross-Entropy Bigram Model:  4.32006642626\n"
     ]
    }
   ],
   "source": [
    "test_string_bgs = \" \".join(test[:10])\n",
    "correct_uni = (addKNUnigram (df_unigram, df_bigram, df_trigram, test_string_bgs))\n",
    "correct_bgs = (addKNBigram (df_unigram, df_bigram, df_trigram, correct_uni[0], correct_uni[1], correct_uni[2]))\n",
    "\n",
    "print ('Perplexity Bigram Model: ', \n",
    "       math.exp(sum(np.log(1/correct_bgs[\"KNSmoothing_BGS\"].dropna(axis=0, how='all')))/len(correct_bgs[\"KNSmoothing_BGS\"].dropna(axis=0, how='all'))))\n",
    "\n",
    "print ('Cross-Entropy Bigram Model: ',\n",
    "      sum(-1*np.log(correct_bgs[\"KNSmoothing_BGS\"].dropna(axis=0, how='all')))/len(correct_bgs[\"KNSmoothing_BGS\"].dropna(axis=0, how='all')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity Trigram Model:  46.097303268521536\n",
      "Cross-Entropy Trigram Model:  3.83075445086\n"
     ]
    }
   ],
   "source": [
    "test_string_tgs = \" \".join(test[:10])\n",
    "correct_uni = (addKNUnigram (df_unigram, df_bigram, df_trigram, test_string_tgs))\n",
    "correct_bgs = (addKNBigram (df_unigram, df_bigram, df_trigram, correct_uni[0], correct_uni[1], correct_uni[2]))\n",
    "correct_tgs = (addKNTrigram (df_unigram, df_bigram, df_trigram, correct_uni[0], correct_bgs, correct_uni[2]))\n",
    "\n",
    "print ('Perplexity Trigram Model: ', \n",
    "       math.exp(sum(np.log(1/correct_tgs[\"KNSmoothing_TGS\"].dropna(axis=0, how='all')))/len(correct_tgs[\"KNSmoothing_TGS\"].dropna(axis=0, how='all'))))\n",
    "\n",
    "print ('Cross-Entropy Trigram Model: ',\n",
    "      sum(-1*np.log(correct_tgs[\"KNSmoothing_TGS\"].dropna(axis=0, how='all')))/len(correct_tgs[\"KNSmoothing_TGS\"].dropna(axis=0, how='all')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
